Chapter 3: Research Methods

What is the probability that you will throw a six? What is the probability it will rain next week? What are the chances that you'll win the National Lottery next week? What is the probability that the things psychologists discover are 'true'? Is scientific 'proof' of something even possible?

The answers to these questions (and more) are in the next few pages. Probably.

60 // Chapter 3 Research methods

Contents

Topic

Page

Research methods recap

62

Correlations

63

Case studies and content analysis

64

Reliability

66

Validity

68

Choosing a statistical test

70

Probability and significance

72

Non-parametric tests: Mann-Whitney and Wilcoxon

74

Parametric tests: Unrelated and related t-tests

76

Tests of correlation: Spearman's and Pearson's

78

Test of association: Chi-Squared

80

Reporting psychological investigations

81

Features of science

82

Practical corner

84

Revision summaries

86

Practice questions, answers and feedback

88

Multiple-choice questions

90

The 50-50-90 rule: Any time you have a 50-50 chance of getting something right, there's a 90% probability you'll get it wrong.

Andy Rooney (US author and commentator)

Chapter 3 Contents // 61

Research methods recap

The specification says...

At AS level students should demonstrate knowledge and understanding of the following research methods, scientific processes and techniques of data handling and analysis, be familiar with their use and be aware of their strengths and limitations. At A level students should also demonstrate knowledge and understanding of inferential testing and be familiar with the use of inferential tests.

Research methods - the story so far ...

AS and Year 1 Specification content Tick off what you already know and would feel confident answering questions on in the exam. Revisit concepts if necessary.

Welcome back to Research methods. Included on this spread is a summary of all the Research methods content that you need to know.

On the right is a recap of what you have already covered in Year 1. Below is a breakdown of the content for this year that is A level only.

Aims: stating aims, the differences between aims and hypotheses. [ ]

Hypotheses: directional and non-directional. [ ]

Variables: manipulation and control of variables, including independent, dependent, extraneous, confounding; operationalisation of variables. [ ]

Control: random allocation and counterbalancing, randomisation and standardisation. Demand characteristics and investigator effects. [ ]

Experimental method. Types of experiment, laboratory and field experiments, natural and quasi-experiments. Experimental designs: repeated measures, independent groups, matched pairs. [ ]

Sampling: the difference between population and sample, sampling techniques including: random, systematic, stratified, opportunity and volunteer; implications of sampling techniques, including bias and generalisation. [ ]

Ethics, including the role of the British Psychological Society's code of ethics; ethical issues in the design and conduct of psychological studies, dealing with ethical issues in research. [ ]

Pilot studies and the aims of piloting. [ ]

Observational techniques. Types of observation: naturalistic and controlled observation, covert and overt observation, participant and non-participant observation. Observational design: behavioural categories, event sampling, time sampling. [ ]

Self-report techniques. Questionnaires, interviews, structured and unstructured. Questionnaire construction, including use of open and closed questions, design of interviews. [ ]

Quantitative and qualitative data, the distinction between qualitative and quantitative data collection techniques. Primary and secondary data, including meta-analysis. [ ]

Descriptive statistics: measures of central tendency: mean, median, mode; calculation of mean, median and mode; measures of dispersion: range and standard deviation; calculation of range. [ ]

Presentation and display of quantitative data: graphs, tables, scattergrams, bar charts, histograms. Distributions: normal and skewed distributions; characteristics of normal and skewed distributions. [ ]

Mathematical content: calculation of percentages, converting a percentage to a decimal, decimal places, converting a decimal to a fraction, using ratios, estimates, significant figures, standard form, order of magnitude calculations, mathematical symbols, substituting values. [ ]

Introduction to statistical testing: the sign test; probability, when to use the sign test; calculation of the sign test. [ ]

The role of peer review in the scientific process. [ ]

The implications of psychological research for the economy. [ ]

Overall, at least 25-30% of the marks in assessments for AS/A level Psychology will be based on assessment of research methods. Although 50% of Paper 2 at A level will assess research methods, it could also be assessed in any other topic on any other paper!

Research methods - still to come

A level only Tick off topics as you complete them.

Case studies. Content analysis and coding. Thematic analysis. [ ]

Reliability across all methods of investigation. Ways of assessing reliability: test-retest and inter-observer; improving reliability. [ ]

Types of validity across all methods of investigation: face validity, concurrent validity, ecological validity and temporal validity. Assessment of validity. Improving validity. [ ]

Correlations. Analysis of the relationship between co-variables. The difference between correlations and experiments. Positive, negative and zero correlations. [ ]

Factors affecting the choice of statistical test, including level of measurement (nominal, ordinal and interval) and experimental design. [ ]

Probability and significance: use of statistical tables and critical values in interpretation of significance; Type I and Type II errors. [ ]

When to use the following tests: Spearman's rho, Pearson's r, Wilcoxon, Mann-Whitney, related t-test, unrelated t-test and Chi-Squared test. [ ]

Reporting psychological investigations. Sections of a scientific report: abstract, introduction, method, results, discussion and referencing. [ ]

Features of science: objectivity and the empirical method; replicability and falsifiability; theory construction and hypothesis testing; paradigms and paradigm shifts. [ ]

Check it

At the end of each chapter in this book (including this one) you will find suggestions for practical investigations. You should carry out as many of these as you can to support your understanding of research methods. Look out for this feature: Apply it in every chapter so you can test your research methods skills.

62 Chapter 3 Research methods

Correlations

The specification says...

Analysis and interpretation of correlation, including correlation coefficients.

Analysis and interpretation of correlation

Correlation is not new to you — you learned about it in Year 1 of the course. Here we will focus on the analysis and interpretation of correlations and correlation coefficients. All included on this spread is a summary of all the Research methods content that you need to know.

Correlations and correlation coefficients

The term correlation refers to a mathematical technique which measures the relationship/association between two continuous variables (properly called co-variables). Such relationships are plotted on a scattergram where each axis represents one of the variables investigated. We shall also see, later in this chapter, how correlations/associations may be analysed using statistical tests.

You will study two statistical tests of correlation (see pages 78-79) each of which, when calculated, produces a numerical value somewhere between -1 and +1 known as the correlation coefficient. This value tells us the strength and direction of the relationship between the two variables.

Working out what a coefficient means

As can be seen below, a value of +1 represents a perfect positive correlation, and a value of -1, a perfect negative correlation.

The closer the coefficient is to +1 or -1, the stronger the relationship between the co-variables is. The closer to zero, the weaker the relationship is. +.50 is as strong a relationship as -.50; the sign just indicates the direction.

However, it should be noted that coefficients that appear to indicate weak correlations can still be statistically significant—it depends on the size of the data set.

Key terms

Correlation: A mathematical technique in which a researcher investigates an association between two variables, called co-variables.

Correlation coefficient: A number between -1 and +1 that represents the direction and strength of a relationship between co-variables.

Scattergrams showing various correlation coefficients.

(Visual representation of scattergrams with values: +1, +.90, +.30, 0, -.40, -.76, -1)

Note that both graphs on the right represent zero correlation even though the distribution of scores is quite different.

Apply it: Methods

Interpretation of correlation coefficients

Questions

What sort of relationship is suggested by the following coefficients? (5 marks) a) -.40 b) +.90 c) +.13 d) -.76 e) 0

What are the strengths and limitations of using correlations in psychological research? (6 marks)

Study tip

At A level you need to be aware of the difference between descriptive statistics and inferential statistics.

Descriptive statistics refers to things like graphs, tables and summary statistics (such as measures of central tendency and measures of dispersion). These are used to identify trends and analyse sets of data.

Inferential statistics refers to the use of statistical tests which tell researchers whether the differences of relationships they have found are statistically significant or not. This helps decide which hypothesis to accept and which to reject. A correlation coefficient is calculated using a statistical test and, as such, is an inferential statistic.

Apply Now

Explain what is meant by 'correlation coefficient'. [2 marks]

Sketch a graph to represent a negative correlation between 'number of people in a room' and 'amount of personal space'. [2 marks]

Using an example, explain what is meant by 'correlation'. [2 marks]

Correlations // 63

Case studies and content analysis

The specification says...

Case studies. Content analysis and coding. Thematic analysis.

Here we look at two ways of investigating human behaviour not considered in Year 1 — case studies and content analysis.

Case studies allow a detailed insight into a single individual, group or institution. It is a method often favoured by researchers who adopt an idiographic approach to the study of human behaviour.

We came across types of observational research in Year 1. Content analysis is a form of observation which analyses the communication that people produce. Anything from a single email or text to a series of films or television programmes may be an appropriate object of study.

Key terms

Case studies: An in-depth investigation, description and analysis of a single individual, group, institution or event.

Content analysis: A research technique that enables the indirect study of behaviour by examining communications that people produce, for example, in texts, emails, TV, film and other media.

Coding: The stage of a content analysis in which the communication to be studied is analysed by identifying each instance of the chosen categories (which may be words, sentences, phrases etc).

Thematic analysis: An inductive and qualitative approach to analysis that involves identifying implicit or explicit ideas within the data. Themes will often emerge once the data has been coded.

Case studies

To study a case in psychology is to provide a detailed and in-depth analysis of an individual, group, institution or event. Case studies often involve analysis of unusual individuals or events, such as a person with a rare disorder or the sequence of events that led to the 2011 London riots (see below). However, case studies may also concentrate on more 'typical' cases, such as an elderly person's recollections of their childhood.

Conducting a case study usually — though not exclusively — involves the production of qualitative data. Researchers may construct a case history of the individual concerned, perhaps using interviews, observations, questionnaires, or a combination of all of these. It is even possible that the person may be subject to experimental or psychological testing to assess what they are (or are not) capable of, and this may produce quantitative data.

Case studies tend to take place over a long period of time (longitudinal) and may involve gathering additional data from family and friends of the individual as well as the person themselves.

Evaluation

Strengths Case studies are able to offer rich, detailed insights that may shed light on very unusual and atypical forms of behaviour. This may be preferred to the more superficial forms of data that might be collected from, say, an experiment or questionnaire. As well as this, case studies may contribute to our understanding of 'normal' functioning. For example, the case of HM (Year 1 book, chapter 2) was significant as it demonstrated 'normal' memory processing — the existence of separate stores in STM and LTM. Case studies may generate hypotheses for future study and one solitary, contradictory instance may lead to the revision of an entire theory — the single pebble that starts an avalanche.

Limitations Generalisation of findings is obviously an issue when dealing with such small sample sizes. Furthermore, the information that makes it into the final report is based on the subjective selection and interpretation of the researcher. Add to this the fact that personal accounts from the participants and their family and friends may be prone to inaccuracy and memory decay, especially if childhood stories are being told. This means that evidence from case studies begins to look a little low in validity.

Apply it: Methods

Gynotikolobomassophobia

Patient X is a gynotikolobomassophobic - he has a morbid fear of women's ear lobes. His fear is so extreme that Patient X finds it impossible to talk to women in social situations (unless their ears are covered) and spends much of his time alone in his home.

A psychologist carrying out a case study of Patient X has conducted detailed interviews with him about his childhood. Patient X has also been encouraged to keep a diary as a record of his everyday experiences. The psychologist has concluded that Patient X's phobia may have been the result of childhood trauma.

Questions

What are the main features of a case study? Refer to Patient X as part of your answer. (4 marks)

Briefly discuss the strengths and limitations of the case study approach. Again, refer to Patient X as part of your discussion. (6 marks)

What ethical issues are associated with the case study approach? (4 marks)

Content analysis

Content analysis is a type of observational research in which people are studied indirectly via the communications they have produced. The forms of communication that may be subject to content analysis are wide-ranging and may include spoken interaction (such as a conversation or speech/presentation), written forms (such as texts or emails) or broader examples from the media (such as books, magazines, TV programmes or films). The aim is to summarise and describe this communication in a systematic way so overall conclusions can be drawn.

Coding and quantitative data Coding is the initial stage of content analysis. Some data sets to be analysed may be extremely large (such as the transcripts of several dozen lengthy interviews) and so there is a need to categorise this information into meaningful units. This may involve simply counting up the number of times a particular word or phrase appears in the text to produce a form of quantitative data. For instance, newspaper reports may be analysed for the number of times derogatory terms for people with mental health issues are used, such as 'crazy' or 'mad'. Another example would be TV adverts which may be examined to see how often men and women are depicted in professional roles (at work) or familial roles (at home) (which is similar to a study carried out by Adrian Furnham and Elena Farragher (2000) - see page 164 for more details).

Thematic analysis and qualitative data Thematic analysis is a form of content analysis but the outcome is qualitative. The main process involves the identification of themes. A theme in content analysis refers to any idea, explicit or implicit, that is recurrent — in other words, which keeps 'cropping up' as part of the communication being studied. These are likely to be more descriptive than the coding units described above. For instance, people with mental health issues may be misrepresented in newspapers as 'a threat to the well-being of our children' or as 'a drain on the resources of the NHS'. Such themes may then be developed into broader categories, such as 'control', 'stereotyping' or 'treatment of people with mental health issues'. Once the researcher is satisfied that the themes they have developed cover most aspects of the data they are analysing, they may collect a new set of data to test the validity of the themes and categories. Assuming these explain the new data adequately, the researcher will write up the final report, typically using direct quotes from the data to illustrate each theme.

Evaluation

Strengths Content analysis is useful in that it can circumnavigate (a posh word for get around) many of the ethical issues normally associated with psychological research. Much of the material that an analyst might want to study, such as TV adverts, films, personal ads in the newspaper or on the internet, etc., may already exist within the public domain. Thus there are no issues with obtaining permission. Such communications have the benefit of being high in external validity, and may access data of a sensitive nature provided the authors consent to its use. We have also seen that content analysis is flexible in the sense that it may produce both qualitative and quantitative data depending on the aims of the research.

Limitations People tend to be studied indirectly as part of content analysis so the communications they produce are usually analysed outside of the context within which it occurred. There is a danger (similar to case studies above) that the researcher may attribute opinions and motivations to the speaker or writer that were not intended originally. To be fair, many modern analysts are clear about how their own biases and preconceptions influence the research process, and often make reference to these as part of their final report (see the idea of reflexivity on page 95). However, content analysis may still suffer from a lack of objectivity, especially when more descriptive forms of thematic analysis are employed.

Apply it: Methods

Toilet humour

Several studies in psychology have involved qualitative analysis of the content of latrinalia — that is, the graffiti often seen scribbled on toilet walls.

A study by Nicholas Matthews et al. (2012) involved the analysis of 1,200 instances of graffiti gathered from toilet walls in US bars. Graffiti was coded according to a number of distinct categories: sexual references, socio-political (religion, politics, race, etc.), entertainment (music, TV), physical presence (the writing of one's name for instance), love/romance and scatological (for example, reference to defecation). Graffiti was also classified in terms of whether it was interactive (a response to other graffiti) or independent (a stand-alone comment).

Matthews et al. found that males composed significantly more sexual and physical presence graffiti, whilst females authored more romantic and interactive graffiti.

Question Explain how this investigation illustrates some of the strengths and limitations of content analysis. (6 marks)

Apply it: Methods

Analysing driving behaviour

A researcher was interested to know whether there is a gender difference in driving behaviour and decided to conduct a content analysis of film clips of male and female drivers.

Question Explain how the researcher might have carried out content analysis to analyse the film clips of driver behaviour. (4 marks)

Apply it: Methods

How to conduct a content analysis

Content analysis, like any observational research, involves design decisions about the following:

Sampling method — how material should be sampled, e.g. time sampling or event sampling.

Recording data — should data be transcribed or recorded, for instance, using video? Should data be collected by an individual researcher or within a team? (See the next spread for a discussion of the importance of inter-rater reliability when conducting content analysis.)

Analysing and representing data — how should material be categorised or coded in order to summarise it? Should the number of times something is mentioned be calculated (quantitative analysis) or described using themes (qualitative analysis)?

Question Explain how, in designing their study of latrinalia, Matthews et al. might have addressed each of the design decisions outlined above. (6 marks)

Check it

Briefly evaluate the use of case studies in psychology. [4 marks]

Explain one limitation of using content analysis to analyse data. [2 marks]

Explain the processes involved in conducting a content analysis. [4 marks]

Case studies and content analysis // 65

Reliability

The specification says...

Reliability across all methods of investigation. Ways of assessing reliability: test-retest and inter-observer; improving reliability.

In everyday life, when we describe someone as reliable, we mean that they are dependable, that we know to expect the same level of behaviour from them every single time. A reliable individual, for instance, is always punctual and never late (or always late and never punctual). A reliable car is one that rarely breaks down and maintains the same level of performance over time. Psychology's version of reliability is pretty similar: to what extent are the tests, scales, surveys or observations or experiments that psychologists use consistent? Any measurement should produce the same data every time it is made, otherwise it is not reliable.

Key terms

Reliability: Refers to how consistent a measuring device is and this includes psychological tests or observations which assess behaviour.

Test-retest reliability: A method of assessing the reliability of a questionnaire or psychological test by assessing the same person on two separate occasions. This shows to what extent the test (or other measure) produces the same answers i.e. is consistent or reliable.

Inter-observer reliability: The extent to which there is agreement between two or more observers involved in observations of a behaviour. This is measured by correlating the observations of two or more observers. A general rule is that if (total number of agreements) / (total number of observations) > +.80, the data has high inter-observer reliability.

Reliability is a measure of consistency. In general terms, if a particular measurement is made twice and produces the same result then that measurement is described as being reliable. A ruler should find the same measurement for a particular object (let's say a chair) every time that object is measured — unless the ruler is broken or, in the words of Phoebe Buffay (Friends, Season 5, Episode 3), 'all the rulers are wrong'. If there is a change in the measurement over time, then we would attribute that change to the object rather than the ruler (someone may have sat on the chair and squashed it).

Similarly, if a test or measure in psychology assessed some 'thing' on a particular day (let's say intelligence), then we would expect the same result on a different day, unless the 'thing' itself had changed. Maybe we tested a different person with a different IQ or the same person's IQ went up a little (or possibly down after watching Friends).

Unlike rulers, psychologists tend not to measure concrete things, like length or height, but are more interested in abstract concepts such as attitudes, aggression, memory and IQ. Can researchers have the same confidence in their psychological tests, observations and questionnaires as most of us — apart from Phoebe that is — have in a ruler?

Ways of assessing reliability

Test-retest Psychologists have devised ways of assessing whether their measuring tools are reliable. The most straightforward way of checking reliability is the test-retest method. This simply involves administering the same test or questionnaire to the same person (or people) on different occasions. If the test or questionnaire is reliable then the results obtained should be the same, or at least very similar, each time they are administered. Note that this method is most commonly used with questionnaires and psychological tests (such as IQ tests) but can also be applied to interviews. There must be sufficient time between test and retest to ensure, say, that the participant/respondent cannot recall their answers to the questions to a survey but not so long that their attitudes, opinions or abilities may have changed. In the case of a questionnaire or test, the two sets of scores would be correlated to make sure they are similar (see below). If the correlation turns out to be significant (and positive) then the reliability of the measuring instrument is assumed to be good.

Inter-observer reliability The phrase 'beauty is in the eye of the beholder' suggests that everyone has their own unique way of seeing the world. This issue is relevant to observational research as one observer's interpretation of events may differ widely from someone else's — introducing subjectivity, bias and unreliability into the data collection process. The recommendation is that would-be observers should not go it alone but instead conduct their observations in teams of at least two. However, inter-observer reliability must be established. This may involve a small-scale trial run (a pilot study) of the observation in order to check that observers are applying behavioural categories in the same way, or a comparison may be reported at the end of a study. Observers obviously need to watch the same event, or sequence of events, but record their data independently. As with the test-retest method, the data collected by the two observers should be correlated to assess its reliability. Note that similar methods would apply to other forms of observation, such as content analysis (though this would be referred to as inter-rater reliability) as well as interviews if they are to be conducted by different people (known as inter-interviewer reliability — which is a bit of a mouthful).

Measuring reliability Reliability is measured using a correlational analysis. In test-retest and inter-observer reliability, the two sets of scores are correlated. The correlation coefficient should exceed +.80 for reliability.

Reliability: it ain't great unless it's... +.8 Statisticians don't write correlations with a leading zero and in reality they always write it as two decimal places but .8 kinda spoils the rhyme!

Apply it: Methods

The correlation 'test'

When assessing test-retest reliability or inter-observer reliability two sets of data will be correlated to see whether they match. The degree of correlation can be measured statistically using a statistical test of correlation such as Spearman's rho (see page 78).

Once the test has been performed on the two sets of data, a correlation coefficient will be calculated. The value of the coefficient must be +.80 or above for data to be judged reliable. Any figure lower than this and researchers must go back to the drawing board so to speak and redesign their test or questionnaire or reassess their observational categories.

Question What would a correlation coefficient of +.95 between the data of two observers suggest? (2 marks)

Improving reliability

Questionnaires As we have seen, the reliability of questionnaires over time should be measured using the test-retest method. Comparing two sets of data should produce a correlation that exceeds +.80 (see facing page). A questionnaire that produces low test-retest reliability may require some of the items to be 'deselected' or rewritten. For example, if some questions are complex or ambiguous, they may be interpreted differently by the same person on different occasions. One solution might be to replace some of the open questions (where there may be more room for [mis]interpretation) with closed, fixed-choice alternatives which may be less ambiguous.

Interviews For interviews, probably the best way of ensuring reliability is to use the same interviewer each time. If this is not possible or practical, all interviewers must be properly trained so, for example, one particular interviewer is not asking questions that are too leading or ambiguous. This is more easily avoided in structured interviews where the interviewer's behaviour is more controlled by the fixed questions. Interviews that are unstructured and more free-flowing are less likely to be reliable.

Observations The reliability of observations can be improved by making sure that behavioural categories have been properly operationalised, and that they are measurable and self-evident (for instance, the category 'pushing' is much less open to interpretation than 'aggression'). Categories should not overlap ('hugging' and 'cuddling' for instance) and all possible behaviours should be covered on the checklist. If categories are not operationalised well, or are overlapping or absent, different observers have to make their own judgements of what to record where and may well end up with differing and inconsistent records. If reliability is low, then observers may need further training in using the behavioural categories and/or may wish to discuss their decisions with each other so they can apply their categories more consistently.

Experiments In an experiment it is the procedures that are the focus of reliability. In order to compare the performance of different participants (as well as comparing the results from different studies) the procedures must be the same (consistent) every time. Therefore in terms of reliability an experimenter is concerned about standardised procedures.

Apply it: Methods

Inter-observer reliability amongst Friends

Two psychology students decided to see whether they could establish inter-observer reliability between themselves. They watched five episodes of Friends and recorded the different types of humour within the programme. Before the study, they agreed on five observational categories of humour: sarcastic, slapstick, sexual/relationship-based, play on words and teasing.

Questions

Invent some data for their observations and put the data in a table. (3 marks)

The students compared their data and found a correlation coefficient of +.64. What does this indicate in terms of the reliability of the two students' data? (2 marks)

What should the students do next to improve the reliability of their observations? (4 marks)

Apply it: Methods

Personality testing

Personality tests in psychology take several forms and are often used in forensic settings to support clinical diagnosis (see the Eysenck Personality Questionnaire, EPQ, on page 330). A more controversial measure of personality is the Rorschach 'inkblot' test.

People are presented with a series of ambiguous inkblot images and are required to say what they see in the pictures. The aim is to reveal the respondent's unconscious motivations and wishes as interpreted by the researcher or therapist. One criticism of the inkblot method is that one scorer may not necessarily produce the same interpretation as another.

Questions

The inkblot test has been criticised by many as an 'unreliable' measure of personality. Why do you think this is? (2 marks)

Explain one way of assessing the reliability of the Rorschach inkblot test. (3 marks)

Apply it: Methods

Ghostly goings on - Part 1

A psychologist wanted to investigate the extent to which people believe in ghosts and devised a questionnaire as a way of assessing this. There were 20 items on the questionnaire in total.

Questions

Outline one way in which the psychologist could have assessed the reliability of the questionnaire. (3 marks)

Following the questionnaire, the psychologist selected a sample of 10 respondents who had completed the questionnaire and then observed their behaviour overnight in a house that was supposedly haunted. Working alongside another observer, the psychologist recorded evidence of a fear reaction to a number of stimuli including a creaking door, a gust of wind and a squeaky floorboard.

State three behavioural categories that could be used to measure the variable 'fear'. (3 marks)

Explain one way in which the researchers could have assessed the reliability of their observations. (3 marks)

Check it

Outline what is meant by 'reliability' in psychological research. [2 marks]

Explain two ways of assessing reliability. [6 marks]

Explain one or more ways of improving reliability. [4 marks]

Reliability // 67

Validity

The specification says...

Types of validity across all methods of investigation: face validity, concurrent validity, ecological validity and temporal validity. Assessment of validity. Improving validity.

Consistency within psychological research is one thing but it is not the only thing. Demonstrating the same (or similar) findings on a number of different occasions is all very well — but what if the 'thing' we are demonstrating each time turns out to be meaningless? Or not what we thought we were demonstrating? This is the issue of validity in psychological research — whether a study, investigation or investigative tool is a legitimate or genuine measure.

Key terms

Validity: The extent to which an observed effect is genuine — does it measure what it was supposed to measure, and can it be generalised beyond the research setting within which it was found?

Face validity: A basic form of validity in which a measure is scrutinised to determine whether it appears to measure what it is supposed to measure — for instance, does a test of anxiety look like it measures anxiety?

Concurrent validity: The extent to which a psychological measure relates to an existing similar measure.

Ecological validity: The extent to which findings from a research study can be generalised to other settings and situations. A form of external validity.

Temporal validity: The extent to which findings from a research study can be generalised to other historical times and eras. A form of external validity.

Types of validity

Validity refers to whether a psychological test, observation, experiment, etc., produces a result that is legitimate. In other words, whether the observed effect is genuine and represents what is actually 'out there' in the real world. This includes whether the researcher has managed to measure what they intended to measure (internal validity). It also refers to the extent to which findings can be generalised beyond the research setting in which they were found (external validity).

It is possible for studies and measures to produce reliable data that is not valid. For instance, a broken set of scales may give a consistent reading of someone's weight which is always 7 lbs more than their actual weight. In this example, the scales are reliable but the weight that is reported is not 'true' so the measurement lacks validity. In psychology, a test that claims to measure intelligence (or IQ) may not measure something true about intelligence — it may simply measure a person's familiarity with IQ tests!

Internal validity Internal validity refers to whether the effects observed in an experiment are due to the manipulation of the independent variable and not some other factor. One major threat to the internal validity of a study is if participants respond to demand characteristics and act in a way that they think is expected. For example, some commentators have questioned the internal validity of Milgram's obedience study claiming that participants were playing along with the experimental situation and did not really believe they were administering shocks, i.e. they responded to the demands of the situation.

External validity Meanwhile, external validity relates more to factors outside of the investigation, such as generalising to other settings, other populations of people and other eras.

Ecological validity is a type of external validity — it concerns generalising the findings from a study to other settings — most particular to 'everyday life' as that is what psychologists are interested in studying. The concept of ecological validity is often misunderstood because people think it is about the 'naturalness' of a study: a more natural setting should mean the findings from the study can be generalised to everyday life (high ecological validity). A lab is an artificial setting and therefore people think that the results of lab research should have low ecological validity because people don't behave naturally in a lab. However, this isn't quite true. If the task that is used to measure the dependent variable in an experiment is not like everyday life (i.e. low mundane realism) this has lower ecological validity. For example, a researcher might give people a list of words to remember to assess memory and could do this in a shopping mall — this would be a field study. However, in this case the setting doesn't make the findings more realistic. The fact that we are using a word list makes the findings of the study lack ecological validity. This means we must look at all sorts of aspects of the research set-up in order to decide whether findings can be generalised beyond the particular research setting.

Temporal validity Temporal validity is the issue of whether findings from a particular study, or concepts within a particular theory, hold true over time. Critics have suggested that high rates of conformity within the original Asch experiments were a product of a particularly conformist era in recent American history (the 1950s). Some of Freud's concepts, such as the idea that females experience penis envy, are deemed to be outdated, sexist and a reflection of the patriarchal Victorian society within which he lived.

Study tip

We have seen how the debate about whether findings from lab studies have ecological validity is often oversimplified. Both Asch's and Milgram's studies might be said to have high ecological validity as they involved processes that often occur in everyday life (conformity and obedience). However, the tasks that participants had to complete within these studies (comparing line lengths and administering electric shocks) were not things people would normally be asked to do. Better to say then that the studies had low mundane realism as the experimental set-up did not mirror everyday life.

Ways of assessing validity

One basic form of validity is face validity: whether a test, scale or measure appears 'on the face of it' to measure what it is supposed to measure. This can be determined by simply 'eyeballing' the measuring instrument or by passing it to an expert to check.

The concurrent validity of a particular test or scale is demonstrated when the results obtained are very close to, or match, those obtained on another recognised and well-established test. A new intelligence test, for instance, may be administered to a group of participants and the IQ scores they achieve may be compared with their performance on a well-established test (such as the Stanford-Binet test). Close agreement between the two sets of data would indicate that the new test has high concurrent validity; and close agreement is indicated if the correlation between the two sets of scores exceeds +.80.

Improving validity

Experiments In experimental research, validity is improved in many ways. For example, using a control group means that a researcher is better able to assess whether changes in the dependent variable were due to the effect of the independent variable. For instance, in a study looking at the effectiveness of a therapy, a control group who did not receive therapy means that the researcher can have greater confidence that improvement was due to effects of the therapy rather than, say, the passage of time. Experimenters may also standardise procedures to minimise the impact of participant reactivity and investigator effects on the validity of the outcome. The use of single-blind and double-blind procedures is designed to achieve the same aim. You may remember that in a single-blind procedure participants are not made aware of the aims of a study until they have taken part (to reduce the effect of demand characteristics on their behaviour). In a double-blind study, a third party conducts the investigation without knowing its main purpose (which reduces both demand characteristics and investigator effects and thus improves validity).

Questionnaires Many questionnaires and psychological tests incorporate a lie scale within the questions in order to assess the consistency of a respondent's response and to control for the effects of social desirability bias. Validity may be further enhanced by assuring respondents that all data submitted will remain anonymous.

Observations Observational research may produce findings that have high ecological validity as there may be minimal intervention by the researcher. This is especially the case if the observer remains undetected, as in covert observations, meaning that the behaviour of those observed is likely to be natural and authentic. In addition, behavioural categories that are too broad, overlapping or ambiguous may have a negative impact on the validity of the data collected.

Qualitative research Qualitative methods of research are usually thought of as having higher ecological validity than more quantitative, less interpretative methods of research. This is because the depth and detail associated with case studies and interviews, for instance, is better able to reflect a participant's reality. However, a researcher may still have to demonstrate the interpretive validity of their conclusions. This is the extent to which the researcher's interpretation of events matches that of their participants. This can be demonstrated through such things as the coherence of the researcher's narrative and the inclusion of direct quotes from participants within the report. Validity is further enhanced through triangulation — the use of a number of different sources as evidence, for example, data compiled through interviews with friends and family, personal diaries, observations, etc.

Apply it: Methods

Ghostly goings on - Part 2

A psychologist wanted to investigate the extent to which people believe in ghosts and devised a questionnaire as a way of assessing this. There were 20 questions in total.

Questions

Explain what is meant by 'validity'. Refer to the investigation above in your answer. (3 marks)

Explain two ways in which the psychologist could have improved the validity of the investigation above. (4 marks)

Threats to validity

The following are threats to validity that we came across in Research methods in Year 1 — though some will apply to particular forms of research more than others. Identify each from the definitions below:

Any variable, other than the IV, that may have an effect on the DV if it is not controlled. These are essentially nuisance variables that do not vary systematically with the IV. (1 mark)

Any variable, other than the IV, that may have affected the DV so we cannot be sure of the true source of changes to the DV. They vary systematically with the IV. (1 mark)

Any cue from the researcher or the research situation that may be interpreted by participants as revealing the true purpose of the investigation. (1 mark)

Any effect of the researcher's behaviour (conscious or unconscious) on the research outcome. This may include everything from the design of the study to the selection of, and interaction with, participants. (1 mark)

A question which, because of the way it is phrased, suggests a certain answer that may influence the response of the participant. (1 mark)

Did you get what you were aiming for? One of the concerns for psychologists trying to improve the validity of their research studies is that their expectations may influence the behaviour of their participants. When assessing concurrent validity, the correlation coefficient between the two sets of scores must exceed +.80. Now where have we seen that before? It ain't great unless it's +.8

Check it

Outline what is meant by 'concurrent validity' in psychological research. [2 marks]

Explain the difference between ecological validity and temporal validity. [4 marks]

Explain two ways of assessing validity. [6 marks]

Explain one or more ways of improving validity. [4 marks]

Validity // 69

Choosing a statistical test

The specification says...

Factors affecting the choice of statistical test, including level of measurement (nominal, ordinal and interval) and experimental design. When to use the following tests: Spearman's rho, Pearson's r, Wilcoxon, Mann-Whitney, related t-test, unrelated t-test and Chi-Squared test.

Quantitative (numerical) data can be summarised using descriptive statistics which include measures of central tendency, measures of dispersion, graphs and charts. Although these are useful, they do not tell us whether the differences or correlations psychologists find are statistically significant (explained on the next spread); this is the job of statistical tests.

Key terms

Statistical tests (also called inferential tests): Used in psychology to determine whether a significant difference or correlation exists (and consequently, whether the null hypothesis should be rejected or retained).

Sign test: A statistical test for a difference in scores between related items (e.g. the same participant tested twice). Data should be nominal level or better.

Levels of measurement: Quantitative data can be classified into types or levels of measurement, such as nominal, ordinal and interval.

Spearman's rho: A test for a correlation when data is at least ordinal level.

Pearson's r: A parametric test for a correlation when data is at interval level.

Wilcoxon: A test for a difference between two sets of scores. Data should be at least ordinal level using a related design (repeated measures).

Mann-Whitney: A test for a difference between two sets of scores. Data should be at least ordinal level using an unrelated design (independent groups).

Related t-test: A parametric test for a difference between two sets of scores. Data must be interval level with a related design, i.e. repeated measures or matched pairs.

Unrelated t-test: A parametric test for a difference between two sets of scores. Data must be interval level with an unrelated design, i.e. independent groups.

Chi-Squared: A test for an association (difference or correlation) between two variables or conditions. Data should be nominal level using an unrelated (independent) design.

Choosing a statistical test

Statistical testing In Year 1 you had a brief introduction to the concept of statistical testing using the example of the sign test. You will recall that a statistical test is used to determine whether a difference or an association/correlation found in a particular investigation is statistically significant — that is, more than could have occurred by chance. The outcome of this has implications for whether we accept or reject the null hypothesis — but we shall return to this on the next spread. For now, we need to consider which statistical test is used under what circumstances. There are three factors used to decide this:

Whether a researcher is looking for a difference or correlation.

In the case of a difference, what experimental design is being used.

The level of measurement.

These criteria are summarised in the table below.

Decision 1. Difference or correlation? The first thing to consider when deciding which statistical test to use relates to the aim or purpose of the investigation — namely, is the researcher looking for a difference or correlation. This should be obvious from the wording of the hypothesis. In this context, 'correlation' can include investigations that are looking for an association (see the Chi-Squared test on page 80).

Decision 2. Experimental design? (Note that if the investigation is looking for a correlation, rather than a difference, experimental design is not an issue) You should remember from Year 1 that there are three types of experimental design: independent groups, repeated measures and matched pairs. The last two of these are referred to as related designs. In a repeated measures design, the same participants are used in all conditions of the experiment. In a matched pairs design, participants in each condition are not the same but have been 'matched' on some variable that is important for the investigation which makes them 'related'. For this reason, both designs are classed as related. As participants in each condition of an independent groups design are different, this design is unrelated. Thus, the researcher chooses from two alternatives here: related or unrelated.

Choosing a statistical test.

Test of difference

Unrelated design

Related design

Test of association or correlation

Nominal data

Chi-Squared

Sign test

Chi-Squared

Ordinal data

Mann-Whitney

Wilcoxon

Spearman's rho

Interval data

Unrelated t-test

Related t-test

Pearson's r

Note that Chi-Squared is a test of both difference and association/correlation. Data items must be unrelated. Also note that the three tests on the blue background are parametric tests (the two forms of t-test and Pearson's r).

Study tip

You will need to memorize the table above so you know which test to use under what circumstances. If you are memorizing the table exactly as it looks here, the following mnemonic might help you remember the sequence of the tests (the first letter in each of the words in the sentence corresponds to the first letter of the stats test):

Carrots Should Come Mashed With Swede Under Roast Potatoes

Decision 3. Levels of measurement? Quantitative data can be divided into different levels of measurement and this is the third factor influencing the choice of statistical test. There are three levels of measurement: nominal, ordinal and interval.

Nominal data Data is represented in the form of categories — hence nominal data is sometimes referred to as categorical data. For example, you can ask everyone in your class if they like psychology. People who say 'yes' are in one group and people who say 'no' are in the other group. Nominal data is discrete in that one item can only appear in one of the categories. For example, if you asked people to name their favourite football team their vote only appears in one category (nominal data can have more than two groups).

Ordinal data Data is ordered in some way. An example of ordinal data would be asking everyone in your class to rate how much they like psychology on a scale of 1 to 10 where 1 is 'do not like psychology at all' and 10 is 'absolutely love psychology'. Ordinal data does not have equal intervals between each unit (unlike in interval data, below). For instance, in our example it would not make sense to say that someone who rated psychology an 8 enjoys it twice as much as someone who gave it a 4. Ordinal data also lacks precision because it is based on subjective opinion rather than objective measures. In our example, what constitutes a '4' or an '8' for the people doing the rating may be quite different. In the case of an IQ test the questions are derived from a view of what constitutes intelligence rather than any universal measurement. Questionnaires, psychological tests and so on do not measure something 'real' (i.e. they are not observable physical entities whereas, for example, reaction times and height are 'real'). Questionnaires etc. measure psychological constructs. For these reasons, ordinal data is sometimes referred to as 'unsafe' data because it lacks precision. Due to its unsafe nature, ordinal data is not used as part of statistical testing. Instead, raw scores are converted to ranks (i.e. 1st, 2nd, 3rd, etc.) and it is the ranks — not the scores — that are used in the calculation (see pages 74-75 and 78 for tests using ordinal data).

Interval data In contrast to ordinal data above, interval data is based on numerical scales that include units of equal, precisely defined size. In this sense it is better than ordinal data because more detail is preserved (and ordinal is better than nominal level). Think of the kinds of things you would use to take measurements with in maths or other sciences, such as a stopwatch, a thermometer or weighing scales. These are public scales of measurement that produce data based on accepted units of measurement (time, temperature, weight). So, for instance, if we recorded how long it took each of our students to complete a written recall test in psychology, we would have collected interval data. Interval data is the most precise and sophisticated form of data in psychology and is a necessary criterion for the use of parametric tests (see right).

Table showing levels of measurement and their relation to the appropriate measures of central tendency and measures of dispersion

Level of measurement

Measure of central tendency

Measure of dispersion

Nominal

Mode

n/a

Ordinal

Median

Range

Interval

Mean

Standard deviation

Note that the range and standard deviation cannot be calculated on nominal data as such data is in the form of frequencies. It is not appropriate to use the mean or the standard deviation for ordinal data as the intervals between the units of measurement are not of equal size.

Apply it: Methods

Which level of measurement?

Identify whether the following would produce data at the nominal, ordinal or interval level.

Time taken to sort cards into categories. (1 mark)

People's choice of the Sun, The Times or the Guardian. (1 mark)

Participants' sense of self-worth, estimated on a scale of 1-10. (1 mark)

Judges in a dancing competition giving marks for style and presentation. (1 mark)

Participants' reaction to aversive stimuli measured using a heart rate monitor. (1 mark)

A set of medical records classifying patients as either 'chronic', 'acute' or 'not yet classified'. (1 mark)

Study tip

Some of the data produced in psychology is quite difficult to classify. For example, should you treat number of words recalled in a memory test as interval or ordinal data? Strictly speaking, this would only be interval data if the words are all of equal difficulty (so the units of measurement are all equivalent). This would be very difficult to achieve as some words will always be more memorable than others! For this reason, it is probably safer to treat number of words recalled as ordinal data and rank the set of scores accordingly. But you should always provide your reasoning when deciding which level of measurement is appropriate.

Apply it: Methods

Parametric tests

The related t-test, unrelated t-test and Pearson's r are collectively known as parametric tests. Parametric tests are more powerful and robust than other tests. If a researcher is able to use a parametric test they will do so, as these tests may be able to detect significance within some data sets that non-parametric tests cannot.

There are three criteria that must be met in order to use a parametric test:

Data must be interval level — parametric tests use the actual scores rather than ranked data.

The data should be drawn from a population which would be expected to show a normal distribution for the variable being measured. Variables that would produce a skewed distribution are not appropriate for parametric tests.

There should be homogeneity of variance: the set of scores in each condition should have similar dispersion or spread. One way of determining variance is by comparing the standard deviations in each condition. If they are similar, a parametric test may be used. In a related design it is generally assumed that the two groups of scores have a similar spread.

Question If a researcher compared two related sets of data and was looking to see if they were different, why would it be preferable to use a related t-test instead of a Wilcoxon? (2 marks)

Check it

Identify and explain the difference between two levels of measurement in psychological research. [4 marks]

Identify three factors that influence the choice of statistical test. [3 marks]

Explain two requirements that need to be met to use an unrelated t-test. [4 marks]

Choosing a statistical test // 71Probability and significance

The specification says...

Probability and significance: use of statistical tables and critical values in interpretation of significance; Type I and Type II errors.

All statistical tests end with a number — the calculated value. This number is crucial in determining whether the researcher has found a result that is statistically significant, and consequently, whether they should accept the alternative or null hypothesis.

To understand how statistical tests work requires an understanding of the related concepts of probability and significance.

Key terms

Probability: A measure of the likelihood that a particular event will occur where 0 indicates statistical impossibility and 1 statistical certainty.

Significance: A statistical term that tells us how sure we are that a difference or correlation exists. A significant result means that the researcher can reject the null hypothesis.

Critical value: When testing a hypothesis, the numerical boundary or cut-off point between acceptance and rejection of the null hypothesis.

Type I error: The incorrect rejection of a true null hypothesis (a false positive).

Type II error: The failure to reject a false null hypothesis (a false negative).

What is the probability of two people in a football match sharing the same birthday? There are 23 people on the pitch (including the referee). The chance that any two people will have the same birthday is 1 in 365. If all 23 people shook hands with each other, there would be 253 handshakes. This equates to the number of pairs of people who could potentially share the same birthday. $253/365 = 0.69$. The probability of two people in a football match sharing the same birthday is 69%, i.e. well over half. Most people are surprised by how high this is!

Probability and significance

The null hypothesis

Researchers begin their investigations by writing a hypothesis. This may be directional or non-directional depending how confident the researcher is in the outcome of the investigation. Here is an example of a hypothesis (you may remember it from the Year 1 book):

After drinking 300 ml of SpeedUpp, participants say more words in the next five minutes than participants who drink 300 ml of water.

This is sometimes referred to as an alternative hypothesis (or $H_1$ for short) because it is alternative to the null hypothesis ($H_0$). The null hypothesis states there is no difference between the conditions:

There is no difference in the number of words spoken in five minutes between participants who drink 300 ml of SpeedUpp and participants who drink 300 ml of water.

The statistical test determines which hypothesis is 'true' and thus whether we accept or reject the null hypothesis.

Levels of significance and probability

Actually, 'true' is probably the wrong word. Statistical tests work on the basis of probability rather than certainty. All statistical tests employ a significance level — the point at which the researcher can claim to have discovered a large enough difference or correlation within the data to claim an effect has been found. In other words, the point at which the researcher can reject the null hypothesis and accept the alternative hypothesis.

The usual level of significance in psychology is 0.05 (or 5%). This is properly written as $p \le 0.05$ (where $p$ stands for probability).

This means the probability that the observed effect (the result) occurred when there is no effect in the population is equal to or less than 5%. This means that even when a researcher claims to have found a significant difference/correlation, there is still up to 5% chance that it isn't true for the target population from which the sample was drawn.

Psychologists can never be 100% certain about a particular result as they have not tested all members of the population under all possible circumstances! For this reason, psychologists have settled upon a conventional level of probability where they are prepared to accept that results may have occurred by chance.

Study tip

People often refer to the concepts of probability and chance in everyday life. We might surmise that the 'chance' of rain is around 50/50, that our favourite football team has a 'good chance' of winning on Saturday, or that we have 'no chance' of winning the National Lottery (the actual statistical probability is about 1 in 14 million).

In psychological research the 5% significance level ensures that, in the case of a significant result, there is equal to or less than a 5% chance that this happened if the null hypothesis was true (i.e. there is no real effect in the population). However, in these circumstances, it is not correct to state that we can be '95% certain' that the result did not occur by chance. If you think about it, the phrase '95% certain' is a contradiction in terms — we can only ever be 100% certain of anything — and statistical testing deals with probabilities not certainties.

Apply it: Methods

Drug testing

A researcher is testing the effectiveness of a new drug that relieves the symptoms of anxiety disorder — Anxocalm. The researcher is comparing two groups of people with anxiety — one group will complete a course of Anxocalm and the other group will be given a placebo. There is a possibility that the drug may cause mild side effects in those who take it (such as a headache and nausea). For this reason, the researcher can only test the drug once on human participants. The researcher has decided to use the 1% level when testing for significance.

Question Explain why the researcher has decided to use the 1% level of significance on this occasion. (2 marks)

Use of statistical tables

Calculated and critical values

Once a statistical test has been calculated, the result is a number — the calculated value (sometimes called the observed value). To check for statistical significance, the calculated value must be compared with a critical value — a number that tells us whether or not we can reject the null hypothesis and accept the alternative hypothesis.

Each statistical test has its own table of critical values, developed by statisticians. These tables look like very complicated bingo cards (see example on the next spread). For some statistical tests, the calculated value must be equal to or greater than the critical value; for other tests, the calculated value must be equal to or less than the critical value (see the 'Rule of R' below).

Using tables of critical values

How does the researcher know which critical value to use? There are three criteria:

One-tailed or two-tailed test? You use a one-tailed test if your hypothesis was directional and a two-tailed test for a non-directional hypothesis. Probability levels double when two-tailed tests are being used as they are a more conservative prediction.

The number of participants in the study. This usually appears as the $N$ value on the table. For some tests degrees of freedom ($df$) are calculated instead.

The level of significance (or $p$ value). As discussed, the 0.05 level of significance is the standard level in psychological research.

Levels of significance

As discussed on the facing page, the 0.05 level of significance is the standard level in psychological research. Occasionally, a more stringent level of significance may be used (such as 0.01) in studies where there may be a human cost — such as drug trials or one-off studies that could not, for practical reasons, be repeated in future. In all research, if there is a large difference between the calculated and critical values — in the preferred direction — the researcher will check more stringent levels, as the lower the $p$ value is, the more statistically significant the result.

Type I and Type II errors

Due to the fact that researchers can never be 100% certain that they have found statistical significance, it is possible (usually up to 5% possible) that the wrong hypothesis may be accepted.

A Type I error is when the null hypothesis is rejected and the alternative hypothesis is accepted when it should have been the other way round because, in reality, the null hypothesis is true. This is often referred to as an optimistic error or false positive as the researcher claims to have found a significant difference or correlation when one does not exist.

A Type II error is the reverse of the above — when the null hypothesis is accepted but it should have been the alternative hypothesis because, in reality, the alternative hypothesis is true. This is a pessimistic error or false negative.

We are more likely to make a Type I error if the significance level is too lenient (too high) e.g. 0.1 or 10% rather than 5%. A Type II error is more likely if the significance level is too stringent (too low) e.g. 0.01 or 1%, as potentially significant values may be missed. Psychologists favour the 5% level of significance as it best balances the risk of making a Type I or Type II error.

Study tip

As suggested above, it is OK to check more stringent levels of significance as long as the critical value at the 5% level has been checked first to establish significance. However, higher levels of significance such as 10% should be disregarded. At these levels the null hypothesis cannot be rejected — though the hypothesis may be worth pursuing and refining the methodology.

Apply it: Methods

The rule of R

Some statistical tests require the calculated value to be equal to or more than the critical value for statistical significance. For other tests, the calculated value must be equal to or less than the critical value.

The rule of R can help with this. Those statistical tests with a letter 'R' in their name are those where the calculated value must be equal to or moRe than the critical value (note that there is also an 'r' in 'more' which is a further clue!).

Questions

List the statistical tests with a letter R in their name. (1 mark)

List the statistical tests without a letter R. (1 mark)

Apply it: Methods

Pregnancy tests

Pregnancy tests are not 100% reliable so women who suspect they are pregnant are advised to take more than one test in order to make sure.

Question If the result says you are not pregnant — in what way could this be a Type II error? (2 marks)

Study tip

If you are testing a directional hypothesis you may find that your calculated value is significant — but there is a further issue. Are your results in the direction you predicted? If they are not then you must accept the null hypothesis even though the calculated value is significant. Before you ask, you can't just change the original hypothesis! In fact, in such cases this should be obvious when looking at the data and a researcher would not carry out any statistical testing.

Check it

Explain what is meant by a Type I and Type II error. Outline the difference between these two errors. 

$$4 marks$$

Define what is meant by the 'critical value' in statistical testing. 

$$2 marks$$

What is the accepted level of significance in psychological research? 

$$1 mark$$

Probability and significance // 73

Non-parametric tests: Mann-Whitney and Wilcoxon

The specification says...

Students should demonstrate knowledge and understanding of inferential testing and be familiar with the use of inferential tests.

An inferential test is another term for a statistical test. In Year 1 of the course you learned to use a statistical test of difference — the sign test. This spread includes two further statistical tests that are used to determine whether two samples are significantly different: Mann-Whitney and Wilcoxon. In each case a worked example is given so you can understand how the test is calculated and how significance is determined.

Mann-Whitney: A worked example

Why Mann-Whitney?

In this worked example we are looking for a difference between two groups of employers based on their rating of whether a candidate (who had schizophrenia) was suitable for a job interview. There are two independent groups of employers, which means the design is unrelated. Finally, the level of measurement is ordinal as data is based on scores on an 'unsafe' scale (subjective ratings of interview suitability) which are converted to ranks for the purposes of the test.

The aim

A study of the effects of labelling in schizophrenia was conducted to see if there is a difference in someone's perceived 'employability' based on whether they had been diagnosed with schizophrenia in the past. Eighteen employers were shown an application form and asked to rate the candidate in terms of how likely they would be called for an interview, on a scale of 1-20 (where 1 = definitely would not be interviewed and 20 = definitely would be interviewed). All employers saw the same application form; the only difference was that for employers in Group A the form included the phrase 'a person recovering from schizophrenia'. For employers in Group B, that phrase was absent from the form.

The hypotheses

Alternative hypothesis: There is a difference in ratings for 'suitability for an interview' based on whether a job applicant is described as a 'person recovering from schizophrenia' (Group A) or not (Group B) (non-directional, two-tailed).

Null hypothesis: There is no difference in ratings for 'suitability for an interview' based on whether a job applicant is described as a 'person recovering from schizophrenia' (Group A) or not (Group B).

Step 1: The table of ranks

To rank the ratings you need to consider the data from both Groups A and B at the same time (data is given in Table 1 below). The lowest number has a rank of 1. In the case where two data items are the same you add up the rank they would get and give the mean for those ranks. For example the rating of 12 appears four times in the table at rank position 7, 8, 9 and 10 therefore they all are given the rank of 8.5. Where there are a lot of multiple ranks it may help to use a frequency table (see Table 1). Calculate the sum of the ranks for Group A ($R_A$) and for Group B ($R_B$) (see Table 2).

Table 1 Frequency table.

Rating

Frequency

Rank

8

1

1

9

1

2

10

2

3 and 4 (Mean 3.5)

11

2

5 and 6 (Mean 5.5)

12

4

7, 8, 9 and 10 (Mean 8.5)

13

1

11

14

1

12

15

2

13 and 14 (Mean 13.5)

16

1

15

17

2

16 and 17 (Mean 16.5)

18

1

18

Table 2 Calculations table.

Group A participant number

Suitability for interview Rating

Rank

Group B participant number

Suitability for interview Rating

Rank

1

12

8.5

11

16

15

2

10

3.5

12

12

8.5

3

13

11

13

14

12

4

8

1

14

15

13.5

5

12

8.5

15

18

18

6

10

3.5

16

17

16.5

7

11

5.5

17

11

5.5

8

15

13.5

18

17

16.5

9

9

2







10

12

8.5







$N_A = 10$



$R_A = 65.5$

$N_B = 8$



$R_B = 105.5$

Step 2: Working out the value of U

Calculate the smaller value of $U$, which in this case will be Group A (the value of $U$ is now called $U_A$ and the number of participants in Group A is referred to as $N_A$).

$U_A = R_A - \frac{N_A(N_A + 1)}{2} = 65.5 - \frac{10(10 + 1)}{2} = 10.5$

Step 3: The calculated and critical values

The calculated value of $U$ is 10.5. The critical value of $U$ for a two-tailed test at the 0.05 level where $N_A = 10$ and $N_B = 8$ is 17 (see table of critical values, Table 3, left).

Table 3 Critical values of U for a two-tailed test, $p \le 0.05$

NA​

1

2

3

4

5

6

7

8

9

10

$N_B$





















1

-

-

0

0

0

1

1

2

2

3

2

-

-

0

1

2

3

4

5

5

6

3

0

0

1

3

5

6

8

10

11

13

4

0

1

3

5

7

10

12

15

17

20

5

0

2

5

7

11

13

16

19

22

25

6

1

3

6

10

13

17

21

25

29

33

7

1

4

8

12

16

21

26

31

36

41

8

2

5

10

15

19

25

31

37

43

49

9

2

5

11

17

22

29

36

43

49

56

10

3

6

13

20

25

33

41

49

56

63

Calculated value of $U$ must be EQUAL TO OR LESS THAN the critical value in this table for significance to be shown. (Note: The table values provided in the text snippet were partially garbled; I have reconstructed a standard Mann-Whitney table format for clarity based on typical values for $p=0.05$ two-tailed). Textbook Note: Note that, in an independent groups design, the numbers of participants in each group may be different as is the case here — Group A has 10 participants ($N_A = 10$) and Group B has 8 participants ($N_B = 8$).

As the calculated value of $U$ is less than the critical value the result is significant ($p \le 0.05$) and we can reject the null hypothesis and accept the alternative hypothesis: There is a difference in ratings for 'suitability for an interview' based on whether a job applicant is described as a 'person recovering from schizophrenia' (Group A) or not (Group B) ($p \le 0.05$).

Apply it: Methods

What does it all mean?

The investigation described on the left found a significant difference at $p \le 0.05$.

Questions

Explain what is meant by the phrase 'a significant difference was found at $p \le 0.05$'. (2 marks)

What conclusion can be drawn from the investigation described? (2 marks)

Wilcoxon: A worked example

Why Wilcoxon?

In this worked example we are looking for a difference in anger scores before and after using an anger management programme. This is a repeated measures design (i.e. related) as the same participants are assessed before and after receiving treatment. The data is ordinal as anger scores are based on a subjective 'unsafe' self-report questionnaire.

The aim

An investigation in forensic psychology was conducted to assess the effectiveness of a new anger management programme. Twelve teenagers serving time in a young offenders institute for violent crime were involved in the study. At the beginning of the investigation, all the offenders completed a questionnaire to measure their level of anger. This gave each offender an anger score out of 50. The offenders then completed eight intensive sessions of anger management. Following the treatment, the offenders completed the same anger questionnaire. The two sets of scores — before and after treatment — were compared to see if there was a difference.

The hypotheses

Alternative hypothesis: There is a difference in young offenders' scores on an anger questionnaire before and after treatment (non-directional, two-tailed).

Null hypothesis: There is no difference in young offenders' scores on an anger questionnaire before and after treatment.

Step 1: Calculate a difference and rank the difference

This time ranking is done on the difference between the two sets of data. When ranking, the signs are ignored. If the difference is zero the data is not included in the ranking and is deducted from the $N$ value, as below.

Table 4 Calculations table.

Participant

Anger score before treatment

Anger score after treatment

Difference

Rank of difference

1

39

30

+9

7.5

2

42

44

-2

1

3

28

25

+3

3

4

35

32

+3

3

5

32

32

0

-

6

40

30

+10

9

7

50

44

+6

6

8

46

50

-4

5

9

29

20

+9

7.5

10

44

29

+15

10

11

25

28

-3

3

12

38

38

0

-

Step 2: Working out the value of T

The calculated value of $T$ is the sum of the less frequent sign. The less frequent sign is minus, so the sum of the ranks is $1 + 5 + 3$. $T = 9$

Step 3: The calculated and critical values

The calculated value of $T$ is 9. The critical value of $T$ for a two-tailed test at the 0.05 level when $N = 10$ (12 participants minus 2 zero differences) is 8 (see table of critical values, Table 5, right).

Table 5 Critical values of T

Level of significance for a one-tailed test

0.05

0.025

0.01

Level of significance for a two-tailed test

0.10

0.05

0.02

$N=5$

0

-

-

6

2

0

-

7

3

2

0

8

5

3

1

9

8

5

3

10

11

8

5

11

13

10

7

12

17

13

9

13

21

17

12

14

25

21

15

15

30

25

19

Calculated value of $T$ must be EQUAL TO OR LESS THAN the critical value in this table for significance to be shown.

As the calculated value of $T$ is more than the critical value of 8 the result is not significant ($p > 0.05$) and we must accept the null hypothesis: There is no difference in young offenders' scores on an anger questionnaire before and after treatment ($p > 0.05$).

We reject the alternative hypothesis at $p \le 0.05$ (i.e. less than a 5% probability that the results are due to chance) and therefore accept the null hypothesis at $p > 0.05$ (i.e. there was more than a 5% probability the results are due to chance).

Apply it: Methods

Using the critical values table

In a similar investigation, a matched pairs design was used to assess the effectiveness of the anger management programme. 20 offenders were matched on anger score at the beginning of the investigation and one from each pair was allocated either to the treatment condition (eight sessions of anger management) or the control condition (no treatment). Anger scores were assessed at the end of the investigation. The calculated value of $T$ was 6. The hypothesis was non-directional. Note that, in a matched pairs design, the $N$ value is based on the number of pairs (10).

Questions

Is the result significant? Explain your answer. (3 marks)

What conclusion can be drawn from this study? (2 marks)

Check it

A researcher was interested to know whether there was a gender difference in 'enjoyment rating' of A level Psychology students.

Which statistical test would be used to analyse the data? Justify your choice. 

$$4 marks$$

When would a researcher decide to use a Wilcoxon test? Refer to three factors in your answer. 

$$3 marks$$

Non-parametric tests: Mann-Whitney and Wilcoxon // 75

Parametric tests: Unrelated and related t-tests

The specification says...

Students should demonstrate knowledge and understanding of inferential testing and be familiar with the use of inferential tests.

These two difference tests are used when data are interval. They are more powerful than the non-parametric Mann-Whitney and Wilcoxon tests.

Unrelated t-test: A worked example

Why the unrelated t-test?

The unrelated t-test is a test of difference between two sets of data. It is used with interval level data only. When an independent groups design is used, the test selected is the unrelated t-test.

In this worked example, we are looking for a difference in the time taken to complete a jigsaw puzzle between boys and girls. The type of design is independent groups (unrelated) because one group were girls and the other group were boys. The criteria for a parametric test were fulfilled: the level of measurement is interval as time taken to complete a jigsaw puzzle is measured on a 'safe' scale (a scale of public measurement) made up of equal units. It is assumed that the participants are drawn from a normally distributed population and there is homogeneity of variance as the standard deviations in both groups are similar.

The aim

An investigation of gender looked into whether there was a difference in visuo-spatial ability between boys and girls. Ten girls and ten boys took part in the test which involved completing a simple jigsaw puzzle in the shortest time possible. The time it took for each participant was recorded and compared.

The hypotheses

Alternative hypothesis: There is a difference in the time taken by boys and girls to complete a jigsaw puzzle (non-directional, two-tailed).

Null hypothesis: There is no difference in the time taken by boys and girls to complete a jigsaw puzzle.

Step 1: The table of data

In Table 1 below various calculations need to be made for the Group A and B scores: Calculate the sum of the scores for Group A ($\sum X_A$). ($X$ refers to scores in Group A). Square each value in Group A ($X_A^2$) and calculate sum of all squared values ($\sum X_A^2$). Repeat for Group B ($\sum X_B$ and $\sum X_B^2$).

Table 1 Calculations table.

Group A Boys

Time taken (sec) XA​

XA2​

Group B Girls

Time taken (sec) XB​

XB2​

1

64

4096

1

52

2704

2

56

3136

2

89

7921

3

90

8100

3

90

8100

4

55

3025

4

112

12544

5

79

6241

5

84

7056

6

102

10404

6

73

5329

7

80

6400

7

79

6241

8

69

4761

8

64

4096

9

69

4761

9

49

2401

10

80

6400

10

90

8100

$\bar{X}_A = 74.3$

$\sum X_A = 744$

$\sum X_A^2 = 57324$

$\bar{X}_B = 75.2$

$\sum X_B = 782$

$\sum X_B^2 = 60052$

Note: There are slight calculation errors in the original PDF table values (e.g., sums don't perfectly match). The text uses $\sum X_A^2 = 57145$ and $\sum X_B^2 = 60052$ in the formula below.

Step 2: Working out the value of t

$S_A = \sum X_A^2 - \frac{(\sum X_A)^2}{N_A} = 57145 - 55204.9 = 1940.1$ $S_B = \sum X_B^2 - \frac{(\sum X_B)^2}{N_B} = 60052 - 56550.4 = 3501.6$

$t = \frac{(\bar{X}_A - \bar{X}_B)}{\sqrt{(\frac{S_A + S_B}{N_A + N_B - 2}) \times (\frac{N_A + N_B}{N_A N_B})}}$ $t = \frac{(74.3 - 75.2)}{\sqrt{(\frac{1940.1 + 3501.6}{10 + 10 - 2}) \times (\frac{10 + 10}{100})}} = -0.116$

Step 3: The calculated and critical values

The calculated value of $t = -0.116$ (note that $t$ is a negative value because the mean for Group B was larger than Group A. When checking the critical values table ignore the negative sign.) The critical value (in Table 2) for a two-tailed test at the 0.05 level where $df = N_A + N_B - 2 = 18$ is 2.101.

As the calculated value (ignoring the sign) is less than the critical value ($p > 0.05$) the result is not significant and we must accept the null hypothesis: There is no difference between boys and girls in the time taken to complete a jigsaw puzzle ($p > 0.05$).

Table 2 Critical values of t

Level of significance for a one-tailed test

0.05

0.025

Level of significance for a two-tailed test

0.10

0.05

$df = 16$

1.746

2.120

17

1.740

2.110

18

1.734

2.101

19

1.729

2.093

20

1.725

2.086

Calculated value of $t$ must be EQUAL TO OR MORE THAN the critical value in this table for significance to be shown.

Related t-test: A worked example

Why the related t-test?

When a repeated measures design is used the test selected is the related t-test.

Here, we are looking for a difference in the average heart rate before and after treatment (CBT). The type of design is repeated measures (related) because the same participants were tested twice. The level of measurement is interval as measurements of heart rate (beats per minute, bpm) are based on a 'safe' scale (a scale of public measurement) made up of equal units. Let us assume for the purpose of this test that participants were drawn from a normally distributed population and homogeneity of variance between the two data sets is assumed as this is a related design.

The aim

In a study of addiction, researchers investigated the effects of CBT on the physiological arousal of gamblers. Ten participants who were categorised as 'persistent gamblers' were given a six-week course of CBT to change their gambling behaviour. Before treatment, all of the participants played on a fruit machine for 20 minutes whilst their heart rate activity was monitored as a measure of physiological arousal. Following treatment, the same participants played on the same game for the same length of time and their heart rate activity was monitored.

The hypotheses

Alternative hypothesis: There is a reduction in heart rate activity when comparing heart rate before and after CBT (directional, one-tailed).

Null hypothesis: There is no difference in heart rate activity comparing heart rate before and after CBT.

Step 1: The table of data

In Table 3 below, various calculations need to be made for Condition A and B. Calculate the difference ($d$) between scores for Condition A and Condition B. Square each difference ($d^2$). Add up the values in the $d$ column to give the sum of $d$ ($\sum d$). Add up the values in the $d^2$ column to give the sum of $d^2$ ($\sum d^2$).

Table 3 Calculations table.

Participant

Condition A Heart rate (bpm) before treatment

Condition B Heart rate (bpm) after treatment

Difference (d)

d2

1

84

80

4

16

2

71

52

19

361

3

20

55

-35

1225

4

58

8

50

2500

5

58

58

0

0

6

77

70

7

49

7

61

63

-2

4

8

75

69

6

36

9

71

74

-3

9

10

70

61

9

81







$\sum d = 55$

$\sum d^2 = 4281$

Note: The table in the PDF has some illegible or inconsistent values in the visual snippet (e.g. participant 3 and 4 values seem unusual). The text calculation uses $\sum d = 31$ and $\sum d^2 = 269$. I will use the text values for the calculation steps below.

Step 2: Working out the value of t

$t = \frac{\sum d}{\sqrt{\frac{N(\sum d^2) - (\sum d)^2}{N - 1}}}$ $t = \frac{31}{\sqrt{\frac{10(269) - (31)^2}{10 - 1}}}$ $t = \frac{31}{\sqrt{\frac{2690 - 961}{9}}}$ $t = \frac{31}{\sqrt{192.11}} = \frac{31}{13.86} = 2.237$

Step 3: The calculated and critical values

The calculated value of $t$ is 2.237. The critical value of $t$ (in Table 2) for a one-tailed test at the 0.05 level where $df = N - 1 = 9$ is 1.833.

As the calculated value of $t$ is greater than the critical value ($p \le 0.05$) the result is significant and we can reject the null hypothesis and conclude: There is a reduction in heart rate activity when comparing heart rate before and after CBT ($p \le 0.05$).

Apply it: Methods

t-tests and taxi drivers

Read the Maguire et al. taxi driver study on page 40. A different researcher wanted to assess whether there was a change in taxi drivers' hippocampal volume as a result of taking 'The Knowledge' test. They analysed the hippocampal volume of 26 trainee London cabbies before they began studying for the test. After all the drivers had completed their training and taken 'The Knowledge' test, the researchers took the same measurement again.

Questions

Write a directional hypothesis for the study described above. (2 marks)

Which of the two t-tests should be used to analyse the data? Justify your answer. (2 marks)

The calculated value of $t$ was 1.526. Is the result significant? Explain your answer. (3 marks)

What conclusion can be drawn from this study? (2 marks)

Study tip

You might be required to calculate the degrees of freedom ($df$):

Pearson's $r$: $df = N - 2$

Related t-test: $df = N - 1$

Unrelated t-test: $df = N_A + N_B - 2$ ($N_A$ is the number of participants in condition A and $N_B$ is the number of participants in condition B).

Check it

A researcher wanted to know whether A level PE students could throw a ball further than A level Geography students. Which statistical test would be used to analyse the data? Justify your choice. 

$$4 marks$$

When would a researcher decide to use a related t-test? Refer to three factors in your answer. 

$$3 marks$$

Parametric tests: Unrelated and related t-tests // 77

Tests of correlation: Spearman's and Pearson's

The specification says...

Students should demonstrate knowledge and understanding of inferential testing and be familiar with the use of inferential tests.

Both of the tests featured here are looking for a correlation between co-variables rather than a difference between sets of scores. Spearman's can be used with ordinal or interval data. Pearson's test can only be used if the data is interval.

Spearman's rho: A worked example

Why Spearman's rho?

Spearman's is a test of correlation between two sets of values. The test is selected when one or both of the variables are ordinal level (though it can be used with interval data). The type of design is not an issue here as the investigation is correlational rather than experimental.

In this worked example, we are looking for a positive correlation between the attractiveness ratings given to each member of the couples. The level of measurement is ordinal as data is based on scores on an 'unsafe' scale (subjective ratings of attractiveness) which are converted to ranks for the purposes of the test.

The aim

A study of relationships was conducted to investigate the matching hypothesis (Walster et al. 1966, see page 122) which proposes that couples in a long-term relationship tend to be similar in terms of physical attractiveness. Twelve couples were selected for the study. Each partner had their photograph taken and these photographs were placed in a random order so it was not obvious who was in a relationship with whom. The 24 photographs were then given to 20 participants (who had never met any of the couples before). The participants were asked to rate the person in each photograph — out of 20 — in terms of their physical attractiveness. The median attractiveness rating for each photograph was calculated to see if there was a significant correlation between pairs in a couple.

The hypotheses

Alternative hypothesis: There is a positive correlation between ratings of physical attractiveness given to two partners in a relationship. (directional, one-tailed).

Null hypothesis: There is no correlation between ratings of physical attractiveness given to two partners in a relationship.

Step 1: The table of ranks

Rank each set of scores separately in each group/condition (in this case, for each partner in the couple) from lowest to highest. As before, if two or more scores share the same ranks, find the mean of their total ranks.

Step 2: Calculate the difference

Find the difference between each pair of ranks and square the difference (as shown in the table below). Finally add up the squared differences. $\sum$ means 'sum of'.

Calculations table

Median physical attractiveness rating for female (out of 20)

Rank for female partner

Median physical attractiveness rating for male (out of 20)

Rank for male partner

Difference between ranks (d)

d2

12.5

11

12.5

9

2

4

16

12

12

8

4

16

13

12

6.5

2.5

9.5

90.25

8.5

2

14.5

11

-9

81

12

7

15

12

-5

25

10

4.5

7

1

3.5

12.25

11.5

6

13.5

10

-4

16

7

1

15

12

-11

121

9

3

11

7

-4

16

17

14

18.5

14

0

0

18

15

12

8

7

49

10

4.5

13

9.5

-5

25









$\sum d^2 = 296$



Note: The table values in the original PDF are slightly garbled/inconsistent. The text uses $\sum d^2 = 296$ for the calculation.

Step 3: Working out the value of rho

$rho = 1 - \frac{6 \sum d^2}{N(N^2 - 1)}$

$rho = 1 - \frac{6 \times 296}{12(144 - 1)} = 1 - \frac{1776}{1716} = 1 - 1.035$ $rho = -0.035$

Step 4: The calculated and critical values

The calculated value of rho is -0.035. The critical value of rho (in Table 2) for a one-tailed test at the 0.05 level where $N = 12$ is 0.503.

As the calculated value of rho (ignoring the sign) is less than the critical value ($p > 0.05$) the result is not significant and we must accept the null hypothesis: There is no correlation between ratings of physical attractiveness given to two partners in a relationship ($p > 0.05$). In addition the result is actually in the wrong direction (negative rather than positive) and so the hypothesis would not be accepted even if the calculated value was sufficiently large.

Table 2 Critical values of rho

Level of significance for a one-tailed test

0.05

Level of significance for a two-tailed test

0.10

$N = 12$

0.503

Calculated value of rho must be EQUAL TO OR MORE THAN the critical value in this table for significance to be shown.

Apply it: Methods

Calculation

A similar investigation with the same hypothesis was conducted with 19 couples. The sum of the difference between the ranks squared ($\sum d^2$) was calculated to be 1000.

Questions

Substitute the correct values into the formula on the right and calculate rho. (3 marks)

Estimate the value of rho based on the values in the formula for question 1. (1 mark)

Explain whether or not the calculated value of rho in question 1 is significant. (2 marks)

Pearson's r: A worked example

Why Pearson's?

Pearson's is a test of correlation between two sets of values. This test is selected when the data is interval level. The type of design is not an issue here as the investigation is correlational rather than experimental.

In this worked example, we are looking for a positive correlation between the length of time (in days) spent using biofeedback and the reduction in resting heart rate (measured in beats per minute, bpm). The level of measurement is interval as data is based on 'safe' mathematical (public measurement) scales. The investigation meets the criteria for a parametric test.

The aim

An investigation into stress was carried out to see if there is a relationship between the length of time using biofeedback (see page 276) and resting heart rate (bpm). Ten participants experiencing chronic stress who had all been using biofeedback for varying lengths of time were selected for the study.

The hypotheses

Alternative hypothesis: There is a positive correlation between the number of days participants have been using biofeedback and the reduction in their resting heart rate (bpm) (directional, one-tailed).

Null hypothesis: There is no correlation between the number of days participants have been using biofeedback and the reduction in their resting heart rate.

Step 1: The table of data

In Table 3 various calculations need to be made for the $x$ and $y$ scores. Calculate the sum of the scores for $x$ ($\sum x$) and $y$ ($\sum y$). Square each $x$ value and each $y$ value. Calculate $\sum x^2$ and $\sum y^2$. Multiply $x$ and $y$ for each participant. Add these values together $\sum xy$.

Table 3 Calculations table.

Participant

Days spent using biofeedback (x)

x2

Reduction in heart rate (y)

y2

xy

1

6

36

2

4

12

2

7

49

2

4

14

3

15

225

4

16

60

4

22

484

6

36

132

5

8

64

3

9

24

6

32

1024

5

25

160

7

44

1936

2

4

88

8

51

2601

8

64

408

9

62

3844

7

49

434

10

80

6400

8

64

640



$\sum x = 340$

$\sum x^2 = 17108$

$\sum y = 49$

$\sum y^2 = 291$

$\sum xy = 2059$

Note: Table uses values consistent with text calculations.

Step 2: Working out the value of r

$r = \frac{N(\sum xy) - (\sum x)(\sum y)}{\sqrt{[N \sum x^2 - (\sum x)^2][N \sum y^2 - (\sum y)^2]}}$

$r = \frac{10(2059) - (340)(49)}{\sqrt{[10(17108) - (340)^2][10(291) - (49)^2]}}$

$r = \frac{3930}{5314} = \textbf{+0.740}$

Step 3: The calculated and critical values

The calculated value of $r$ is +0.740. The critical value of $r$ (in Table 4) for a one-tailed test at the 0.05 level where $df = N - 2 = 8$, is 0.549.

As the calculated value of $r$ is more than the critical value the result is significant at the 0.05 level and we can reject the null hypothesis and accept the alternative hypothesis: There is a positive correlation in the number of days participants have been using biofeedback and the reduction in their resting heart rate ($p \le 0.05$).

Table 4 Critical values of r

Level of significance for a one-tailed test

0.05

Level of significance for a two-tailed test

0.10

$df = 8$

0.549

Calculated value of $r$ must be EQUAL TO OR MORE THAN the critical value in this table for significance to be shown.

Apply it: Methods

Using the critical values table

A researcher was interested to know if there was a positive correlation between heat and aggression. He made a note of the average temperature in his local town on various days throughout the year. He also recorded the number of violent incidents that were reported in the local newspapers on those days. The researcher used a Pearson's test to analyse his data. The calculated value of $r$ was 0.281. Data for daily temperature and number of violent incidents was recorded for 52 days throughout the year.

Questions

Is the result significant? Explain your answer. (3 marks)

What conclusion can be drawn from this study? (2 marks)

Check it

When would a researcher decide to use a Spearman's rho test? Refer to two factors in your answer. 

$$2 marks$$

When would a researcher decide to use a Pearson's test? Refer to two factors in your answer. 

$$2 marks$$

Tests of correlation: Spearman's and Pearson's // 79

Test of association: Chi-Squared

The specification says...

Students should demonstrate knowledge and understanding of inferential testing and be familiar with the use of inferential tests.

There is one final statistical (inferential) test that you have to study: the Chi-Squared test, which can be used for differences or association. The key feature of Chi-Squared is that each data item is not listed separately but, instead, a frequency count is given. Usually the data is entered in 'cells' (a 2x2 table), but 4x4 or 3x2 etc. can be used. The first number identifies the number of rows and the second number the number of columns. The data in each cell must be independent — imagine that each data item is one person, each person can only be placed in one cell of the contingency table.

Chi-Squared

Why Chi?

Chi-Squared is a test of difference or association. The data is nominal and recorded as a frequency count of the categories. In this worked example, we are looking for a difference in the ability to decentre in children aged 5 and children aged 8. There are two independent groups of children which means the design is unrelated. Finally, the level of measurement is nominal as data is collected in the form of frequencies in two categories — ability to decentre or not.

The aim

A study of cognitive development was conducted to see if there was a difference in children's ability to decentre (see the world from the perspective of another) depending on their age. A group of 5-year-olds and 8-year-olds were given the three mountains task (see page 180) to see whether they could choose a photograph that corresponded to a doll's view rather than their own.

The hypotheses

Alternative hypothesis: More 8-year-olds than 5-year-olds are able to select a photograph that represents a perspective different from their own (directional, one-tailed).

Null hypothesis: There is no difference between the number of 5-year-olds and 8-year-olds who can select a photograph that represents a perspective different from their own.

Step 1: A 2x2 contingency table

Draw a 2x2 contingency table showing the observed frequencies (i.e. the data that was collected) in each cell and calculate the totals for each row, each column and the overall total.

Table 1 Contingency table



5-year-olds

8-year-olds

Totals

Could decentre

6 (cell A)

28 (cell B)

34

Could not decentre

27 (cell C)

9 (cell D)

36

Totals

33

37

70

Step 2: The table of expected frequencies

Expected frequencies ($E$) are now calculated for each of the four cells in the 2x2 table. An expected frequency is the frequency that would be expected if there was no difference between the two groups (i.e. if the age of the child had no effect on their ability to decentre). The expected frequency is calculated for each cell by multiplying the total for the row by the total for the column divided by the grand total. $E = \frac{\text{row total} \times \text{column total}}{\text{grand total}}$

Step 3: Working out the value of $\chi^2$

$\chi^2 = \sum \frac{(O - E)^2}{E}$

Table 2 Calculations table.

Cell

O

E

O−E

(O−E)2

E(O−E)2​

Cell A

6

$34 \times 33 / 70 = 16.0$

-10

100

6.3

Cell B

28

$34 \times 37 / 70 = 18.0$

10

100

5.5

Cell C

27

$36 \times 33 / 70 = 17.0$

10

100

5.9

Cell D

9

$36 \times 37 / 70 = 19.0$

-10

100

5.3











Total = 23.0

Step 4: The calculated and critical values

The calculated value of $\chi^2$ is 23.0. To find the critical value, calculate the degrees of freedom ($df$) by multiplying $(\text{rows} - 1) \times (\text{columns} - 1)$. $df = (2-1) \times (2-1) = 1$. The critical value of $\chi^2$ (in Table 3) for a one-tailed test at the 0.05 level, where $df = 1$, is 2.71.

As the calculated value of $\chi^2$ is more than the critical value ($p \le 0.05$) we can reject the null hypothesis and accept the alternative hypothesis: More 8-year-olds than 5-year-olds are able to select a photograph that represents a perspective different from their own ($p \le 0.05$).

Table 3 Critical values of $\chi^2$

Level of significance for a one-tailed test

0.05

Level of significance for a two-tailed test

0.10

$df = 1$

2.71

Calculated value of $\chi^2$ must be EQUAL TO OR MORE THAN the critical value in this table for significance at the level shown.

Apply it: Methods

Calculating Chi

A researcher wanted to see whether there was an association between age and voting preference in the General Election. One hundred voters were classified as either 'young' (under 25) or 'old' (over 60). Of the 50 'young' voters, 42 voted for the Pro-Zombie Party and 8 for the Anti-Zombie Party. Of the 50 'old' voters, 32 voted for the Anti-Zombie Party and 18 for the Pro-Zombie Party.

Questions

Construct a 2x2 contingency table for the data above. (3 marks)

Calculate the value of $\chi^2$ for the data above. (4 marks)

Explain whether the value of $\chi^2$ you calculated in question 2 is significant. (2 marks)

Suggest one conclusion that could be drawn from your answer to question 3. (2 marks)

Reporting psychological investigations

The specification says...

Reporting psychological investigations. Sections of a scientific report: abstract, introduction, method, results, discussion and referencing.

When psychologists come to write up their research for publication in journal articles, they use a conventional format. On this half-spread we describe each of the sections that make up a scientific report.

Key terms

Abstract: The key details of the research report.

Introduction: A look at past research (theories and/or studies) on a similar topic. Includes the aims and hypothesis of current investigation.

Method: A description of what the researcher(s) did, including design, sample, apparatus/materials, procedure, ethics.

Results: A description of what the researcher(s) found, including descriptive and inferential statistics.

Discussion: A consideration of what the results of a research study tell us in terms of psychological theory.

References: List of sources that are referred to or quoted in the article (e.g. journal articles, books or websites) and their full details.

Sections of a scientific report

Abstract

The first section in a journal article is a short summary/abstract (150-200 words in length) that includes all the major elements: the aims and hypotheses, method/procedure, results and conclusions. When researching a particular topic, psychologists will often read lots of abstracts in order to identify those studies that are worthy of further examination.

Introduction

The introduction is a literature review of the general area of research detailing relevant theories, concepts and studies that are related to the current study. The research review should follow a logical progression — beginning broadly and gradually becoming more specific until the aims and hypotheses are presented.

Method

Split into several subsections, the method should include sufficient detail so that other researchers are able to precisely replicate the study if they wish.

Design — the design is clearly stated, e.g. independent groups, naturalistic observation, etc., and reasons/justification given for the choice.

Sample — information related to the people involved in the study: how many there were, biographical/demographic information (as long as this does not compromise anonymity), the sampling method and target population.

Apparatus/materials — detail of any assessment instruments used and other relevant materials.

Procedure — a recipe-style list of everything that happened in the investigation from beginning to end. This includes a verbatim record of everything that was said to participants: briefing, standardised instructions and debriefing.

Ethics — an explanation of how these were addressed within the study.

Results

The results section should summarise the key findings from the investigation. This is likely to feature descriptive statistics such as tables, graphs and charts, measures of central tendency and measures of dispersion. Inferential statistics should include reference to the choice of statistical test, calculated and critical values, the level of significance and the final outcome, i.e. which hypothesis was rejected. Any raw data that was collected and any calculations appear in an appendix rather than the main body of the report. If the researcher has used qualitative methods of research, the results/findings are likely to involve analysis of themes and/or categories.

Discussion

There are several key elements in the discussion section. The researcher will summarise the results/findings in verbal, rather than statistical, form. These should be discussed in the context of the evidence presented in the introduction and other research that may be considered relevant. The researcher should discuss the limitations of the present investigation and this may include some suggestions of how these limitations might be addressed in a future study. Finally, the wider implications of the research are considered. This may include real-world applications of what has been discovered and what contribution the investigation has made to the existing knowledge-base within the field.

Referencing

Referencing includes full details of any source material cited in the report. Journal references follow the format: author(s), date, article title, journal name (in italics), volume(issue), page numbers. For example: Gupta, S. (1991) Effects of time of day and personality on intelligence test scores. Personality and Individual Differences, 12(11), 1227-1231. Book references take the following format: author(s), date, title of book (in italics), place of publication, publisher. For example: Skinner, B. F. (1953) Science and Human Behaviour. New York: MacMillan. Web references provide source, date, title, weblink and date accessed.

Check it

When would a researcher decide to use a Chi-Squared test? Refer to three factors in your answer. 

$$3 marks$$

Outline what information psychologists should include in an abstract when reporting psychological investigations. 

$$3 marks$$

Identify and outline two sections of a scientific report. 

$$6 marks$$

List four subsections that should be included in the method section of a psychological report. 

$$4 marks$$

Test of association: Chi-Squared and Reporting psychological investigations // 81

Features of science

The specification says...

Features of science: objectivity and the empirical method; replicability and falsifiability; theory construction and hypothesis testing; paradigms and paradigm shifts.

What makes science scientific? And is psychology a science? On this spread we attempt to tackle both of these questions by first describing the key features and assumptions of scientific enquiry. We will then consider to what extent psychology as a social scientific discipline (rather than a natural science) meets these criteria.

Key terms

Objectivity: All sources of personal bias are minimised so as not to distort or influence the research process.

Empirical method: Scientific approaches that are based on the gathering of evidence through direct observation and experience.

Replicability: The extent to which scientific procedures and findings can be repeated by other researchers.

Falsifiability: The principle that a theory cannot be considered scientific unless it admits the possibility of being proved untrue (false).

Theory construction: The process of developing an explanation for the causes of behaviour by systematically gathering evidence and then organising this into a coherent account (theory).

Hypothesis testing: A key feature of a theory is that it should produce statements (hypotheses) which can then be tested. Only in this way can a theory be falsified.

Paradigm: A set of shared assumptions and agreed methods within a scientific discipline.

Paradigm shift: The result of a scientific revolution when there is a significant change in the dominant unifying theory within a scientific discipline.

Features of science

Paradigms and paradigm shifts

The philosopher Thomas Kuhn (1962) suggested that what distinguishes scientific disciplines from non-scientific disciplines is a shared set of assumptions and methods — a paradigm. Kuhn suggested that social sciences (including psychology) lack a universally accepted paradigm and are probably best seen as 'pre-science' as distinct from natural sciences such as biology or physics. Natural sciences are characterised by having a number of principles at their core such as the theory of evolution in biology, or the standard model of the universe in physics. Psychology, on the other hand, is marked by too much internal disagreement and has too many conflicting approaches to qualify as a science and therefore is a pre-science (this view of psychology has been challenged — see below).

According to Kuhn, progress within an established science occurs when there is a scientific revolution. A handful of researchers begin to question the accepted paradigm, this critique begins to gather popularity and pace, and eventually a paradigm shift occurs when there is too much contradictory evidence to ignore. Kuhn cited the change from a Newtonian paradigm in physics towards Einstein's theory of relativity as an example of a paradigm shift.

Theory construction and hypothesis testing

Science tests theories — but what is a theory? A theory is a set of general laws or principles that have the ability to explain particular events or behaviours. Theory construction occurs through gathering evidence via direct observation (see the empirical method). For instance, I may have a hunch that short-term memory has a limited capacity based on the observation that people struggle to remember much when they are bombarded with information. A series of experiments reveals that the average short-term memory span is around 7 (give or take 2) items of information. Let's call this Berry's Law. OK fine, someone else got there first. But this is a good example of a theory as it proposes a simple and economical principle which appears to reflect reality. It provides understanding by explaining regularities in behaviour.

An essential component of a theory is that it can be scientifically tested. Theories should suggest a number of possible hypotheses. For instance, 'Berry's Law' (see, it's catching on) suggests that people will remember 7-digit pastcodes more effectively than 14-digit mobile phone numbers. A hypothesis like this can then be tested (hypothesis testing) using systematic and objective methods to determine whether it will be supported or refuted. In the case of the former, the theory will be strengthened. In the case of the latter, the theory may need to be revised or revisited. The process of deriving new hypotheses from an existing theory is known as deduction.

Falsifiability

Another philosopher of science whose work appeared around the same time as Thomas Kuhn was Karl Popper (1934) who argued that the key criterion of a scientific theory is its falsifiability. Genuine scientific theories, Popper suggested, should hold themselves up for hypothesis testing and the possibility of being proven false. He believed that even when a scientific principle had been successfully and repeatedly tested, it was not necessarily true. Instead it had simply not been proven false — yet! This became known as the theory of falsification. Popper drew a clear line between good science, in which theories are constantly challenged and therefore can potentially be falsified, and what he called 'pseudosciences' which couldn't be falsified.

Those theories that survive most attempts to falsify them become the strongest — not because they are necessarily true — but because, despite the best efforts of researchers, they have not been proved false (which provides them with some strength). This is why psychologists avoid using phrases such as 'this proves' in favour of 'this supports' or 'this seems to suggest' and why, as we have seen, an alternative hypothesis must always be accompanied by a null hypothesis (this allows for falsifying the hypothesis).

Replicability

An important element of Popper's hypothetico-deductive method (described above) is replicability. If a scientific theory is to be trusted, the findings from it must be shown to be repeatable across a number of different contexts and circumstances. Replication has an important role in determining the validity of a finding. We have already discussed the role of replication in determining the reliability of the method used in a study. Replication is also used to assess the validity of a finding — by repeating a study, as Popper suggests, over a number of different contexts and circumstances then we can see the extent to which the findings can be generalised. In order for replicability to become possible, it is vital that psychologists report their investigations with as much precision and rigour as possible, so other researchers can seek to verify their work and verify the findings they have established.

Objectivity and the empirical method

Scientific researchers must strive to maintain objectivity as part of their investigations. In other words, they must keep a 'critical distance' during research. They must not allow their personal opinions or biases to 'discolour' the data they collect or influence the behaviour of the participants they are studying. As a general rule, those methods in psychology that are associated with the greatest level of control, such as lab experiments, tend to be the most objective.

Objectivity is the basis of the empirical method. The word empiricism is derived from the Greek for 'experience' and empirical methods emphasise the importance of data collection based on direct, sensory experience. The experimental method and the observational method are good examples of the empirical method in psychology. Early empiricists such as John Locke saw knowledge as determined only by experience and sensory perception. Thus, according to Locke's view, a theory cannot claim to be scientific unless it has been empirically tested and verified.

Apply it: Methods

Does psychology have a paradigm?

Kuhn's argument was that psychology's lack of an accepted paradigm means it is yet to achieve the status of normal science, and is instead, 'pre-science'. Certainly there are a number of theoretical perspectives in psychology that have suggested quite different ideas and ways of investigating the human subject. However, not all commentators agree with Kuhn's conception of psychology as pre-scientific. For instance, the vast majority of researchers would accept a definition of psychology as the study of mind and behaviour suggesting there is broad agreement. Similarly, it could be argued that psychology has already progressed through several paradigm shifts from Wundt's early structuralism to the dominant cognitive neuroscience model of today. Finally, several researchers including Feyerabend (1975) have suggested that Kuhn's conception of 'proper' science as orderly and paradigmatic is flawed, and that most sciences are in fact characterised by internal conflict, dispute and a refusal to accept new ideas in the face of evidence.

Questions

Choose two approaches in psychology and explain how the main assumptions and methods of enquiry within these two approaches differ. (6 marks)

Use your knowledge of the historical development of psychology to explain how the discipline may have experienced several paradigm shifts. (4 marks)

Apply it: Methods

Psychology as a science: the case for

Scientific psychology lifts everyday understanding of human behaviour above the level of common sense. Critics of psychology may claim it amounts to little more than common sense, but many key findings in psychology are counter-intuitive and not what a common-sense view would predict. By adopting a scientific model of enquiry, psychology gives itself greater credibility by being placed on equal footing with other, more established sciences (despite Kuhn's suggestion that psychology is just a pre-science). The scientific approach in psychology has provided many practical applications that have improved people's lives and challenged/modified dysfunctional behaviour.

Questions

As an example of counter-intuitive findings, explain why Milgram's findings were not what most people would have predicted. (3 marks)

List at least two practical applications of psychology and examine their effectiveness. (2 marks)

Apply it: Methods

Psychology as a science: the case against

Although many psychologists try to maintain objectivity within their research, some of the methods that psychologists use are subjective, non-standardised and unscientific. Science is based on the assumption that it is possible to produce universal laws that can be generalised across time and space. However, this may not be possible in psychology — samples of participants in studies are rarely representative and conclusions drawn may often be influenced by cultural and social norms. Much of the subject matter in psychology cannot be directly observed and must be based on inference rather than objective measurement.

Questions

Provide an example of subjective methods in psychology, with reference to specific studies. (2 marks)

Even when more objective methods are used, explain why objectivity may be much harder to achieve in psychology than in natural sciences, e.g. physics, chemistry. (3 marks)

Why might replicability be harder to achieve in psychology than other sciences? (3 marks)

Explain which psychological approaches this most applies to and why. (2 marks)

Explain why many findings gained from experimental research may lack ecological validity and/or temporal validity. Give some examples. (4 marks)

Explain why the issue of inference is a criticism that may be levelled at the cognitive approach. (1 mark)

Check it

Outline what is meant by 'replicability' and 'falsifiability' in psychological research. 

$$4 marks$$

Outline what is meant by the following terms in scientific research: (a) paradigm, (b) paradigm shifts. 

$$4 marks$$

Briefly discuss the importance of theory construction and hypothesis testing in scientific research. 

$$4 marks$$

Discuss features of science. 

$$16 marks$$

Features of science // 83

Practical corner

The specification says...

Knowledge and understanding of research methods, practical research skills and mathematical skills. These should be developed through ethical practical research activities.

This means that you should conduct practical investigations throughout all topics. Here, we suggest some ways in which you might put your research methods skills into practice. Firstly, in the development of an IQ test which you should then assess in terms of its reliability and validity. Secondly, it's over to you for the ideas and ways to generate data for all those fascinating statistical tests!

Ethics check

We strongly suggest that you complete this checklist before starting:

Do participants know participation is voluntary?

Do participants know what to expect?

Do participants know they can withdraw at any time?

Are individuals' results anonymous?

Have I minimised the risk of distress to participants?

Have I avoided asking sensitive questions?

Will I avoid bringing my school/teacher/psychology into disrepute?

Have I considered all other ethical issues?

Has my teacher approved this?

Apply it: Methods

The maths bit

Imagine you recorded the scores the first time your participants sat the test and compared these with the scores the second time participants sat the test. Spearman's rho was used to work out the relationship between the two sets of scores and a correlation coefficient of +.91 was found.

Questions

Explain why Spearman's rho was used to analyse the relationship between the two sets of scores. (4 marks)

Explain what is meant by 'correlation coefficient'. (2 marks)

Explain what a correlation coefficient of +.91 means in this context. (3 marks)

Imagine you also asked the same participants to complete an established IQ test and compared these scores with the scores on the first test. Again, Spearman's rho was used to analyse the relationship and the correlation coefficient was +.57.

Explain what a correlation coefficient of +.57 means in this context. (3 marks)

Explain what type of validity is being tested in the example above. Justify your answer. (2 marks)

Outline one other way in which you could have assessed the validity of the IQ test. (3 marks)

Practical idea 1: Assessing reliability and validity of an IQ test

On pages 66-69 we talked about the importance psychologists place on designing measuring instruments that are reliable (consistent) and valid (true). Now it's your chance to develop a test of intelligence (IQ) and establish whether it has test-retest reliability and concurrent validity.

The aim of this study is to develop a simple form of IQ test and assess whether it is reliable and valid.

The practical bit

There are many ways of testing intelligence — puzzles, problem-solving exercises and IQ tests (IQ stands for intelligence quotient). Your first task is to develop a simple IQ test.

Designing the test
There are many different types of IQ test. Typically most 'traditional' forms of the test contain questions (items) that assess skills of mathematics, verbal reasoning, spatial awareness and comprehension. In order to get a feel for the kind of IQ test you want to design, it would be worth having a look at a few examples online (be aware though that not all tests are free and some will charge for the results if you take the test). A good free online test can be found at the following link: www.iqtest.dk

If you feel that some of the questions you've seen may be a little difficult to design, you might want to go a different way. Instead of measuring IQ, you could test general knowledge (different from intelligence!), for example of world history, geography, current affairs, literature, etc.

Or your test could be based on one specific knowledge area, such as world football, cinema, pop music or basket weaving. The actual content of the test is not all that important, just so long as there are a decent number of questions (say, around 30-40), and some are easier and some more difficult than others.

Assessing reliability
Once you're reasonably happy with the content of your test (and you've had it checked by a teacher) you are ready to assess its reliability. On page 66 we discussed the concept of test-retest reliability. Reliable measuring instruments should produce the same (or similar) results if they are used again with the same sample of people. In order to assess this, you're going to have to find some willing participants to take the test twice. Between 10 and 20 people should do the trick, but really it's the more the merrier. You need to leave a reasonable period of time between the two tests, maybe a couple of weeks, to reduce the possibility that participants will remember most of the questions (and more importantly the answers!). Details on how to analyse the two sets of scores are included below.

Assessing validity
On page 69 we introduced the idea of concurrent validity. One way of assessing validity is to compare the scores produced on your test with the scores produced on an established test that measures the same variable (intelligence). So you're going to need to find a proven IQ test — or, you could just follow this link to the Stanford-Binet test, a widely recognised and well established test: www.stanfordbinet.net

The same participants who completed your test above are also required to complete the Stanford-Binet test. Make sure you record all the sets of scores.

Statistical analysis
Your final task is to perform two statistical analyses. The first is to determine the correlation between the scores produced on the test the first time the participants take it and the scores produced the second time they take it. This will assess test-retest reliability.

The second analysis is to calculate the correlation between the scores on the test (the first time participants take it) and their scores on the Stanford-Binet test. This will assess concurrent validity.

Spearman's rho
Both of the calculations above require you to use Spearman's rho. You will need to remind yourself of the steps involved in this test (page 78) in order to work out the correlation coefficient in each case. Finally, remember that in order to establish reliability and validity your correlation coefficients need to exceed +.80 (it ain't great unless it's .8).

Practical idea 2: Devising practical ideas for research

The aim of this practical is to encourage you to develop research ideas for each of the statistical tests described on pages 74-80.

The introductory bit

By this stage, you should have experience of designing and carrying out practical investigations on your own or in a small group. Your task is to design a further seven investigations (some of which you may want to carry out) to generate data for all of the statistical tests you have come across in this chapter. This is no mean feat and will require many of the research methods skills you have developed as well as some careful thought.

An investigation for Chi-Squared
Chi-Squared is the most flexible of the statistical tests and can be used to test for an association (relationship) or a difference. The level of measurement is nominal so data needs to be represented in the form of categories. You might want to analyse some difference between the genders as the number of male and female students (in your year group for instance) can easily be counted and recorded. Is there a difference between the number of male and female students who do and do not study psychology, for instance? Remember, you'll need to construct a contingency table (a 2x2 table) of the relevant data so you are able to calculate the difference/association you are looking for.

An investigation for Mann-Whitney
This test of difference requires ordinal data and independent groups. For this reason, it is a good test for comparing the opinions or ratings given by two different groups. Is someone's enjoyment of their A level programme influenced by whether they study psychology or not? Alternatively, Mann-Whitney is appropriate for measuring the difference in performance of groups on a non-interval scale test. Would Year 13s outperform Year 12s on the IQ test you devised on the left?

An investigation for Wilcoxon
The same criteria for Mann-Whitney apply to Wilcoxon except data is drawn from related rather than unrelated samples. If participants were matched pairs in the investigations above, Wilcoxon would be suitable because that is a related design. Alternatively, participants could be asked to give a rating for something before and after they are given new information. Do students' ratings of their perceived ability in psychology change after they are told that everyone in last year's class got an A grade? (Ed — this may or may not be true of course!)

Investigations for Spearman's and Pearson's
Both of these are tests of correlation so there is no experimental design to consider here. The distinction between the two types of test is the level of measurement. If you are looking for a correlation between co-variables where at least one of these co-variables is measured at an ordinal level then you select Spearman's (for example, whether the number of siblings a student has is related to their self-rating of patience). If both sets of data are interval level then it's the parametric equivalent, Pearson's (for example, whether time taken to complete a jigsaw puzzle is related to age).

Investigations for the t-tests
Both forms of t-test analyse the difference between two sets of scores and require interval data for their use. The distinction between the related and unrelated t-test hinges on the type of experimental design. If this is independent groups, then the unrelated t-test is used, for instance, investigating whether A level PE students can throw a tennis ball further than non-PE students. In the case of matched pairs or repeated measures designs, the related t-test is used. For example, investigating whether the presence of an audience affects how far students throw a tennis ball.

Parametric versus non-parametric assumptions
Remember that the two t-tests and Pearson's are parametric tests and certain assumptions must be met for their use. You must be sure that the data you are using is interval data. Generally speaking, if data has been recorded using some specialist mathematical measuring device (such as a tape measure in the example above) it is likely to be interval data — but check with your teacher if you are unsure. Using students from your school is likely to ensure that your sample is drawn from a normally distributed population. Finally, if you are using a related design, homogeneity of variance is assumed. If your design is unrelated, you will need to work out the standard deviations for each condition to make sure they are similar (see page 71 for a discussion of parametric criteria).

Stats test checklist

Make sure you:

Write an operationalised alternative and null hypothesis at the beginning of the investigation (you'll need to accept one and reject the other at the end).

Decide on a directional or non-directional hypothesis (and, consequently, a one-tailed or a two-tailed test) (See page 73).

Design a well-controlled and ethical investigation — see 'Ethics check' on facing page and check this with your teacher if you intend to carry it out.

Choose the correct statistical test.

Select the correct critical value using an appropriate level of significance (usually 5%).

You won't have to calculate any of the statistical tests on the left in the exam but you might want to work out a couple just for — er — fun. (Also, it will help your understanding of maths and statistics in general!)

The maths bit

In the Year 1 book we gave a list of the mathematical skills you will be expected to demonstrate.

Overall, at least 10% of the marks in assessments for Psychology will require the use of mathematical skills and this is included in the requirement that at least 25-30% in total will involve research methods.

84 Chapter 3 Research methods
Practical corner // 85

Revision summaries

Correlations

Revisiting the analysis of co-variables

Relationship between two continuous co-variables.

Analysis and interpretation

Correlation coefficient: Represents strength and direction of relationship.

Working out what a coefficient means: The closer the coefficient is to -1 or +1, the stronger the relationship. +.50 is as strong as -.50, the sign just tells us the direction.

Case studies and content analysis

Case studies

Detailed analysis of an unusual individual or event, e.g. the London riots. May also be typical behaviours.

Characteristics: May involve a case history. Qualitative (e.g. interviews) and quantitative data (e.g. psychological tests). Tend to be longitudinal.

Strengths: Insight into unusual cases, e.g. HM may provide understanding of normal functioning. Generate hypotheses for future studies.

Limitations: Generalisation from small samples is a problem and conclusions based on subjective interpretation of the researcher, plus subjective data from participants.

Content analysis

A form of observation in which communication is studied indirectly.

Coding and quantitative data: Data must be categorised into meaningful units (and then analysed by counting words, etc.).

Thematic analysis and qualitative data: Recurrent ideas (themes) that keep 'cropping up' in the communication are identified and described.

Strengths: Fewer ethical issues, high external validity, flexible approach because can be adapted.

Limitations: Information may be studied out of context and be subjective. Reflexivity aims to address the issue of bias.

Reliability

Reliability

A measure of consistency. Any measurement should produce the same result unless the thing it is measuring has changed.

Ways of assessing reliability

Test-retest: The same test is administered to the same person on different occasions and results compared.

Inter-observer reliability: Observers compare data in a pilot study or at end of actual study to make sure behavioural categories are consistently applied.

Measuring reliability: Two sets of scores should correlate at least +.80 for reliability.

Improving reliability

Questionnaires: If a questionnaire has low test-retest reliability, some items may need to be changed to closed questions as these may be less ambiguous.

Interviews: Should avoid questions that are leading or ambiguous and ensure interviewers are trained.

Observations: Behavioural categories should be properly operationalised, more training may be needed.

Experiments: Standardised procedures ensures consistency when testing different participants.

Validity

Types of validity

Whether a test, scale, etc. produces a legitimate result which represents behaviour in the real world. A measure of truth.

Internal and external validity: Whether something measures what it was designed to measure, and whether findings can be generalised.

Ecological validity: The extent to which findings can be generalised from one setting to other settings. Mundane realism of task may affect ecological validity.

Temporal validity: Whether findings from a study hold true over time.

Ways of assessing validity

Face validity: Does a test measure what it is supposed to 'on the face of it'?

Concurrent validity: Do results match with a previously established test?

Improving validity

Experimental research: Use of a control group. Standardised procedures. Single-blind and double-blind procedures.

Questionnaires: Use of lie scales and anonymity to reduce social desirability bias.

Observations: Use covert observations so behaviour more authentic, also use well-defined behavioural categories.

Qualitative research: Depth and detail may increase validity but further enhanced though triangulation.

Choosing a statistical test

Statistical testing

Statistical tests tell us whether results are significant or due to chance. Determine whether we can accept or reject the null hypothesis.

Decisions

Decision 1. Difference or correlation? Correlation includes tests of association (Chi-Squared).

Decision 2. Experimental design? Related (repeated measures or matched pairs) or unrelated (independent groups).

Decision 3. Level of measurement?

Nominal data: Data represented in the form of categories, e.g. counting how many students like psychology or don't like psychology in your class.

Ordinal data: Ordered data, but unequal intervals. Can be placed in rank order, e.g. rating your liking of psychology on a scale of 1-10.

Interval data: Based on numerical and public scales of measurement with units of equal size, e.g. length, temperature.

Parametric tests

Used with interval level data, normal distribution expected and satisfies homogeneity of variance (standard deviation squared).

Probability and significance

Key concepts

Probability and significance: Psychological research works on probabilities rather than certainties.

The null hypothesis: The null hypothesis states no difference between conditions. Statistical tests determine whether this should be accepted or rejected.

Levels of significance and probability: The significance level is the point at which the researcher can accept the alternative hypothesis (usually 5% in psychology).

Use of statistical tables

Calculated and critical values: The calculated value must be compared with a critical value to determine significance.

Using tables of critical values:

Is a one-tailed or two-tailed test required?

What is the N or df value?

Which level of significance is required (e.g. 0.05)?

Rule of R: Tests with a letter R in their name are those where the calculated value must be equal to or more than the critical value.

Type I and Type II errors

Type I: The incorrect rejection of a true null hypothesis. More likely if significance level is too lenient (e.g. 10%). An optimistic error.

Type II: The incorrect acceptance of a false null hypothesis. More likely if significance level is too stringent (e.g. 1%). A pessimistic error.

Non-parametric tests

Mann-Whitney U

Test of difference between two sets of data.

Unrelated design.

Data at least ordinal level.

Wilcoxon

Test of difference between two sets of data.

Related design.

Data at least ordinal level.

Parametric tests

Unrelated t-test

Test of difference between two sets of data.

Unrelated design.

Data at interval level.

Parametric test - data drawn from a population with an expected normal distribution and both data sets have a homogeneity of variance.

Related t-test

Test of difference between two sets of data.

Related design.

Data at interval level.

Parametric test - data drawn from a population with an expected normal distribution and both data sets have a homogeneity of variance.

Tests of correlation

Spearman's rho

Test of correlation between co-variables.

Data at least ordinal level.

Pearson's r

Test of correlation between co-variables.

Data at interval level.

Parametric test - data drawn from a population with an expected normal distribution and both data sets have a homogeneity of variance.

Test of association

Chi-Squared

Test of difference between two sets of data or association between co-variables.

Data is independent.

Nominal data.

Reporting psychological investigations

Sections of a scientific report

Abstract: A short summary (200 words) of the key elements in the report.

Introduction: Literature review, including aim and hypothesis.

Method: Includes design, sample, apparatus/materials, procedure, ethics.

Results: Descriptive and inferential statistics. Raw data in appendix.

Discussion: Analysis of results, links to previous research, limitations and wider implications.

Referencing: List of sources (journal articles, books, web sources). Generally includes author(s), date, title, volume/page numbers/publisher/source.

Features of science

Key concepts

Paradigms and paradigm shifts: Scientific subjects have a shared set of assumptions (Kuhn) and a scientific revolution occurs when there is a paradigm shift.

Theory construction and hypothesis testing: Theory construction occurs through gathering evidence from direct observation. A theory should produce testable hypotheses, thus permitting the validity of the theory to be assessed.

Falsifiability: Scientific theories must hold themselves up for hypothesis testing and the possibility of being proved false.

Replicability: If a scientific theory is to be trusted (i.e. valid), its findings must be shown to be repeatable across time and context. The methods used should also be repeatable, i.e. reliable.

Objectivity and the empirical method: Scientists must minimise all sources of personal bias and gather evidence through direct observation and experience.

86 Chapter 3 Research methods
Revision summaries // 87

Practice questions, answers and feedback

Question 1
A call centre company is conducting research to see whether the type of music customers listen to whilst they are on hold affects how long they will remain on hold. The first 200 customers who phoned the call centre on a Monday morning were told they had to be transferred to another department and were placed on hold (in reality, they were not being transferred, they were simply being placed on hold and the time it took each customer to put the phone down was recorded).

During the time on hold, 100 customers were played classical music, and the other 100 customers were played pop music. The difference in average time spent listening to classical music and pop music on hold before putting the phone down was analysed.

(a) Write a suitable non-directional hypothesis for the investigation above. (2 marks)

Morticia's answer: The hypothesis would be "There is a difference between the customers who listened to classical music and the customers who listened to pop music".

Feedback: A difference in what? The DV is not made clear.

Luke's answer: The customers who heard classical music waited longer on hold than the customers who heard pop music.

Feedback: Unfortunately, a directional hypothesis is offered by Luke.

Vladimir's answer: The kind of music that is played affects how long people stay on hold.

Feedback: Both variables are stated and the hypothesis is non-directional.

(b) Identify which sampling technique was used to recruit participants in the study. Explain your answer. (2 marks)

Morticia's answer: The researcher used an opportunity sample because it was just the most convenient people.

Feedback: 'Most convenient people' is an odd phrase but just about enough.

Luke's answer: Opportunity sample. They were the most willing and available.

Feedback: Participants were not given the option to be 'willing', but still a sufficient answer.

Vladimir's answer: Volunteer sample because they were willing to take part.

Feedback: Vladimir's answer has no relevance.

(c) Outline one limitation of the sampling technique you have identified in (b). (2 marks)

Morticia's answer: Using an opportunity sample is not always the most representative.

Feedback: A limitation is stated but more detail is required.

Luke's answer: This sample may be biased because it is a narrow sample of people who happened to be at home to answer their phones.

Feedback: A more thorough answer but the link to the stem is inaccurate.

Vladimir's answer: Volunteers may be more willing than people generally which makes them unrepresentative.

Feedback: Vladimir has fully addressed a limitation of the sampling method he identified earlier. He is not penalised for making a wrong answer in the previous part.

(d) Identify which statistical test could have been used to analyse the difference in average time spent on hold in the research study in question 1. Justify your answer. (3 marks)

Morticia's answer: It would be a t-test for unrelated samples. This is because the study is independent groups and the data is interval.

Feedback: Morticia has produced a reasonable answer, though it is good practice to link the answer more clearly to the stem (as in Vladimir's excellent answer below). She hasn't stated that a difference test is needed.

Luke's answer: It would be a Mann-Whitney test because there are two groups of participants and we can't be sure that parametric criteria are satisfied.

Feedback: Luke's answer lacks key elements — the type of design is not named, nor is the fact that a difference is being tested for.

Vladimir's answer: Unrelated t-test because there were two groups of participants (those who heard classical or heard pop), we are looking for a difference between the groups, the data is interval (number of seconds) and parametric criteria would be satisfied.

Feedback: Vladimir has got it right.

(e) After the results had been analysed, a significant difference was found at the 0.05 level. Explain what this means in relation to the research study above. (3 marks)

Morticia's answer: It means that there is a 5% probability that the results were due to chance.

Feedback: Morticia's answer is the only one here that contains relevant material. The other two answers are too vague. None of the answers refers to which hypothesis would be retained and which rejected, and what this would mean in the context of the investigation.

Luke's answer: 0.05 is the same as 5% which is the level of certainty we have that there was a difference.

Vladimir's answer: The difference was 95% certain.

On this spread we look at some typical student answers to questions. The comments provided indicate what is good and bad in each answer. Learning how to produce effective question answers is a SKILL. Read pages 387-397 for guidance.

As soon as customers put the phone down in the research study described on the facing page, they were called back and debriefed.

(f) Write a debriefing statement that could have been read out to customers who were played pop music whilst on hold. (4 marks)

Morticia's answer: I am ringing you to say that the phone call you just received was part of a research study run by Littletown University. We played music for each participant — either classical or pop — to see how long they would stay on the phone. If you'd like to know the final results of the study I can send them to you. Do you have any questions?

Feedback: The aim is clearly stated in Morticia's debrief and the reference to any questions and knowledge of the final results is appropriate. She could perhaps have asked if the data could still be used.

Luke's answer: Hello, my name is Luke. I am ringing to tell you that the phone call you just received was part of a psychological study where we were investigating whether people were more willing to stay on hold if they listened to classical or pop music. We hope you were not distressed by the experience. Thank you for your time.

Feedback: The aim of the investigation is clearly stated but Luke should have asked for permission to use the data. The reference to 'distress' is rather vague and unnecessary.

Vladimir's answer: This is a follow-up call to the one you just received to tell you that it was part of a psychological study. We hope it didn't upset you in any way or take up too much of your time. If you have any complaints or would like to withdraw your data then let us know now. Otherwise thank you and I'll be in touch again to tell you the results if you wish.

Feedback: A good appreciation of relevant ethical issues in Vladimir's debrief. However, this answer needs more detailed reference to the aim of the study and relevant conditions.

Customers were asked if they would complete a structured interview about their experience of call centres. Those who agreed were called back and the interview was conducted over the phone.

(g) Briefly discuss one limitation of using a structured interview in this study. (3 marks)

Morticia's answer: A structured interview does not give the interviewer any opportunity to ask questions arising from the conversation during the phone interview.

Feedback: Morticia's and Vladimir's answers are briefly stated and require further elaboration perhaps through contrast with unstructured interviews.

Luke's answer: One limitation of a structured interview is that the interviewer can't stray from the listed questions, for example to explain the question. In this interview they might want to ask some different questions from those that were on the list.
That said, the data from structured interviews is much easier to analyse as it is more focused than that produced in unstructured interviews.

Feedback: Luke's answer is much better for this reason. It also includes relevant counterargument which can be used as effective elaboration in a 'briefly discuss' question. However there is no reference to the study — Morticia does manage to include some context ('during the phone interview') as required in the question.

Vladimir's answer: It lacks the flexibility of an unstructured interview.

(h) Explain how the reliability and validity of an interview in this study could have been assessed. (6 marks)

Morticia's answer: The reliability could be assessed by asking the same interviewer to repeat the interview a second time with the same people and compare the answers to see if they were consistent. Validity could be checked in terms of face validity to see if the questions looked like they were assessing thoughts and feelings about call centres.

Feedback: Morticia's answer is detailed and accurate. The ways of assessing reliability and validity are well explained and relevant to an interview. There is no requirement here to relate the answer to the particular study but that would be a way to provide useful detail.

Luke's answer: For reliability you could use test-retest and repeat the interviews a second time. For validity you could use concurrent validity and compare with another interview.

Feedback: Luke's answer is less successful. Two relevant methods are named but the description of these lack key details.

Vladimir's answer: One way to assess reliability is using the test-retest method where the same test (or questions) are given a second time to the same person. Validity could be ecological validity. The ecological validity is good because these were people just answering their phones in everyday life.

Feedback: Vladimir's first point is relevant — though generic — but the material in the latter half of the question is not really 'assessing' validity.

88 Chapter 3 Research methods
Practice questions, answers and feedback // 89

Multiple-choice questions

Correlations

All correlation coefficients fall somewhere between:
(a) -1 and +1
(b) 0 and 1
(c) -1 and 0
(d) -10 and +10

As the number of people in a room increases, personal space decreases is an example of a:
(a) Positive correlation.
(b) Negative correlation.
(c) Zero correlation.
(d) Curvilinear relationship

As the daily temperature goes down, people's mood rating decreases is an example of a:
(a) Positive correlation.
(b) Negative correlation.
(c) Zero correlation
(d) Curvilinear relationship

Correlations are plotted on a:
(a) Bar chart.
(b) Histogram.
(c) Line graph.
(d) Scattergram.

Case studies and content analysis

Case studies tend to take place over a long period of time. This is referred to as:
(a) Longitudinal research.
(b) Timescale research.
(c) Hexagonal research.
(d) Temporal research.

Which of the following is a strength of case studies?
(a) The final report is not based on the subjective interpretation of the researcher.
(b) Generalisation of findings is usually possible with small sample sizes.
(c) Construct validity.
(d) Study of unusual cases may contribute to our understanding of normal functioning.

Categorising information into meaningful units is known as:
(a) Generalising.
(b) Correlational.
(c) Separating.
(d) Coding.

Recognising the part that one's own biases play in the research process is called:
(a) Reflexivity.
(b) Rigidity.
(c) Reactivity.
(d) Reliability.

Reliability

Reliability is a measure of:
(a) Complexity.
(b) Conformity.
(c) Consistency.
(d) Clarity.

If two or more observers are collecting data they should first establish:
(a) Construct validity.
(b) Test-retest reliability.
(c) Inter-observer reliability.
(d) External reliability.

If participants are given the same questionnaire twice, for test-retest reliability the correlation must exceed:
(a) +.70
(b) +.80
(c) +.90
(d) +1.00

Reliability is most often achieved in which type of interview?
(a) Structured.
(b) Unstructured.
(c) Semi-structured.
(d) Television.

Validity

Which of the following does not describe validity?
(a) Whether a test is legitimate.
(b) Whether a test is genuine.
(c) Whether a finding can be generalised.
(d) Whether a test is consistent.

'The extent to which findings from a research study can be generalised to other times' is a definition of:
(a) Concurrent validity.
(b) Temporal validity.
(c) Face validity.
(d) Internal validity.

'The extent to which a psychological measure relates to an existing similar measure' is a definition of:
(a) Concurrent validity.
(b) Temporal validity.
(c) Face validity.
(d) Internal validity.

Validity can be improved through the use of a number of different sources as evidence. This is referred to as:
(a) Reticulation.
(b) The square peg principle.
(c) Circle theory.
(d) Triangulation.

Choosing a statistical test

Which of the following is not one of the criteria when deciding on a statistical test?
(a) Whether the hypothesis is directional or non-directional.
(b) The level of measurement.
(c) Whether the researcher is looking for a difference or relationship.
(d) The experimental design.

Which of the following tests would be used when looking for a correlation with interval level data?
(a) Mann-Whitney.
(b) Chi-Squared.
(c) Pearson.
(d) Spearman.

Which of the following tests would be used with nominal data?
(a) Related t-test.
(b) Wilcoxon.
(c) Sign test.
(d) Unrelated t-test.

Which of the following is not one of the three criteria required for use of a parametric test?
(a) Data at interval level.
(b) A test of relationship rather than difference.
(c) Data must be drawn from a normally distributed population.
(d) Homogeneity of variance.

Probability and significance

A probability of 1 would indicate:
(a) Statistical certainty.
(b) Statistical impossibility.
(c) Statistical significance.
(d) Statistical likelihood.

Which of the following is associated with a Type II error?
(a) An optimistic error.
(b) Incorrect rejection of the null hypothesis.
(c) Incorrect acceptance of the alternative hypothesis.
(d) Made more likely when the significance level is set too low.

The usual level of significance in psychological research is:
(a) 1%
(b) 5%
(c) 10%
(d) 50%

Which of the following is not required when consulting a critical values table?
(a) Knowing the level of measurement.
(b) Knowing whether the test is one-tailed or two-tailed.
(c) Knowing the N value/degrees of freedom.
(d) Knowing the level of significance.

Non-parametric tests: Mann-Whitney and Wilcoxon

Mann-Whitney would be used when the researcher has used:
(a) An independent groups design.
(b) A repeated measures design.
(c) A matched pairs design.
(d) A quasi-experiment.

Wilcoxon would be used when the researcher has used:
(a) An independent groups design.
(b) Stratified sampling.
(c) A matched pairs design.
(d) A quasi-experiment.

Mann-Whitney and Wilcoxon are used when:
(a) The data is interval.
(b) The data is nominal.
(c) The data is ratio.
(d) The data is ordinal or interval.

Which test would be used to analyse differences on an attitude questionnaire between males and females?
(a) Mann-Whitney.
(b) Wilcoxon.
(c) Neither (a) nor (b).
(d) Either (a) or (b).

Parametric tests: Unrelated and related t-tests

Why are parametric tests superior to non-parametric tests?
(a) They use the actual data collected rather than ranks.
(b) They are more powerful and robust than other tests.
(c) They are more likely to detect significance in data sets.
(d) All of the above.

An unrelated t-test would be used when the researcher has used:
(a) An independent groups design.
(b) A repeated measures design.
(c) A matched pairs design.
(d) A quasi-experiment.

t-tests are used when:
(a) The data is interval.
(b) The data is nominal.
(c) The data is ordinal.
(d) The data is independent.

Which test would be used to analyse an association between gender and smoking behaviour?
(a) Wilcoxon.
(b) Related t-test.
(c) Neither (a) nor (b).
(d) Either (a) or (b).

Tests of correlation: Spearman's and Pearson's

Spearman's would be used when:
(a) The data is at least interval.
(b) The data is at least nominal.
(c) The data is at least ratio.
(d) The data is at least ordinal.

Pearson's would be used when:
(a) The data is at least interval.
(b) The data is at least nominal.
(c) The data is at least ordinal.
(d) There is no data.

Spearman's and Pearson's are both used to analyse:
(a) A correlation.
(b) A difference.
(c) Dispersion.
(d) Central tendency.

Which test would be used to analyse the relationship between temperature and heart rate?
(a) Mann-Whitney.
(b) Pearson's.
(c) Neither (a) nor (b).
(d) Either (a) or (b).

Test of association: Chi-Squared and Reporting psychological investigations

Chi-Squared is used when:
(a) The data is interval.
(b) The data is nominal.
(c) The data is ratio.
(d) The data is ordinal.

Observed frequencies are recorded in a:
(a) Consistency table.
(b) Continuity table.
(c) Contingency table.
(d) Constancy table.

Which of these is defined as a 150-200 word summary of the major elements of a scientific report?
(a) The abstract.
(b) The design.
(c) The discussion.
(d) The references.

Which of the following would not appear in the method section of a scientific report?
(a) Sample.
(b) Procedure.
(c) Discussion.
(d) Apparatus/materials.

Features of science

Kuhn referred to a shared set of assumptions and methods as a:
(a) Parametric.
(b) Parapax.
(c) Paradigm.
(d) Parasite.

Popper argued that scientific progress occurs through a process of:
(a) Ramification.
(b) Amplification.
(c) Falsification.
(d) Maternalisation.

When all sources of personal bias are minimised so as not to influence the research process.
(a) Replicability.
(b) Generalisability.
(c) Falsifiability.
(d) Objectivity.

'Collecting data through direct sensory experience' refers to:
(a) Existentialism.
(b) Essentialism.
(c) Eclecticism.
(d) Empiricism.

MCQ answers

Correlations: 1A, 2B, 3A, 4D

Case studies and content analysis: 1A, 2D, 3D, 4A

Reliability: 1C, 2C, 3B, 4A

Validity: 1D, 2B, 3A, 4D

Choosing a statistical test: 1A, 2C, 3B, 4B (Correction: Q3 asks "Which of the following tests would be used with nominal data?" - Correct answer is Chi-Squared or Sign Test. Option 3 is listed as 'Sign Test' in original text list. The answer key says 3B which is Wilcoxon in the Q3 list? Wait. Let's re-read the options. Q3 options: (a) Related t-test, (b) Wilcoxon, (c) Sign test, (d) Unrelated t-test. The answer key says 3B. This seems like a textbook error, as Wilcoxon uses Ordinal data. Sign test uses Nominal. However, I must transcribe word for word.)

Correction Note: In the "Multiple-choice questions" section, Q3 for "Choosing a statistical test" has options: (a) Related t-test (b) Wilcoxon (c) Sign test (d) Unrelated t-test. The Answer Key says "Choosing a statistical test 1A, 2C, 3D, 4B". Wait, looking at the image provided for the key: "Choosing a statistical test 1A, 2C, 3B, 4B".

Probability and significance: 1A, 2D, 3B, 4A

Non-parametric tests: Mann-Whitney and Wilcoxon: 1A, 2C, 3D, 4A

Parametric tests: Unrelated and related t-tests: 1D, 2A, 3A, 4C

Tests of correlation: Spearman's and Pearson's: 1D, 2A, 3A, 4B

Test of association: Chi-Squared and Reporting psychological investigations: 1B, 2C, 3A, 4C

Features of science: 1C, 2C, 3D, 4D

90 Chapter 3 Research methods
Multiple-choice questions // 91